{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones Finales\n",
    "\n",
    "- **Análisis Predictivo**: El modelo de red neuronal puede automatizar el proceso de calificación a partir de texto no estructurado, facilitando el monitoreo del sentimiento de los clientes a través de diversas plataformas.\n",
    "\n",
    "- **Análisis de Sentimiento y Subjetividad**: Permite identificar el nivel general de satisfacción de los clientes y distinguir entre comentarios objetivos y opiniones subjetivas.\n",
    "\n",
    "- **Modelado de Temas y Análisis Basado en Aspectos**: Brinda información detallada sobre áreas específicas de interés o preocupación, ayudando a la aerolínea a enfocarse en aspectos críticos a mejorar.\n",
    "\n",
    "- **Resúmenes de Reseñas**: Ofrece una representación concisa de los comentarios de los clientes, útil para que los responsables de la toma de decisiones obtengan una visión rápida sin tener que profundizar en cada reseña individual.\n",
    "\n",
    "### Propuesta de Valor para Flybondi\n",
    "\n",
    "- **Mejora en la Satisfacción del Cliente**: Al comprender los sentimientos de los clientes y los aspectos que consideran importantes, Flybondi puede tomar decisiones basadas en datos para mejorar la experiencia del cliente.\n",
    "\n",
    "- **Gestión de la Reputación**: El monitoreo del sentimiento en distintas plataformas ayuda a gestionar de manera proactiva la imagen pública de la aerolínea.\n",
    "\n",
    "- **Planificación Estratégica**: Los conocimientos obtenidos del modelado de temas y el análisis basado en aspectos pueden orientar las estrategias de marketing y las mejoras en los servicios.\n",
    "\n",
    "- **Asignación de Recursos**: Enfocarse en las áreas que impactan significativamente en la satisfacción del cliente permite optimizar la utilización de los recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>source_language</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>es</td>\n",
       "      <td>terrible terrible bad lose hotel day way back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>one bad fly fly hundred airline possibly bad b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>terrible service flight first delay 25 minute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>rubbish low cost airline buy 6 ticket via book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885david_r885</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>refund cancel flight refund cancel flight basi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  rating  relevance_score source_language  \\\n",
       "0        22fortinero       1              0.0              es   \n",
       "1         23russellv       2              0.0              en   \n",
       "2            4family       1              0.0              en   \n",
       "3  5travellers602013       1              0.0              en   \n",
       "4      885david_r885       1              0.0              en   \n",
       "\n",
       "                                              review  \n",
       "0  terrible terrible bad lose hotel day way back ...  \n",
       "1  one bad fly fly hundred airline possibly bad b...  \n",
       "2  terrible service flight first delay 25 minute ...  \n",
       "3  rubbish low cost airline buy 6 ticket via book...  \n",
       "4  refund cancel flight refund cancel flight basi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../data/en_cleaned_with_lemmatized_reviews.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Predictive Analysis: a network to predict sentiment\n",
    "\n",
    "aca hay una red neuronal que usa la funcion de activación logistica y es capaz de predecir que rating le da el reviewer basado en el texto. Usa Adam como optimizador ya que es el mejorcito del mercado.\n",
    "\n",
    "### resultados en 5 epocas\n",
    "- adam relu [mae= 0.17,mse=  0.045, r2<0] \n",
    "- adam linear [mae=0.17, mse=0.046, r2<0]\n",
    "- adam sigmoid: [mae=0.11 , mse=0.026, r2=0.74] \n",
    "- Usando el relevance score [mae= 0.099, mse=0.026, r2= 0.74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 17:15:44.339054: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-09 17:15:44.339181: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: JGirod\n",
      "2024-11-09 17:15:44.339194: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: JGirod\n",
      "2024-11-09 17:15:44.339443: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 535.183.1\n",
      "2024-11-09 17:15:44.339470: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: 535.183.1\n",
      "2024-11-09 17:15:44.339477: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:249] kernel version seems to match DSO: 535.183.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - loss: 0.0949 - mae: 0.2753 - mse: 0.0949 - val_loss: 0.0414 - val_mae: 0.1589 - val_mse: 0.0414\n",
      "Epoch 2/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0282 - mae: 0.1243 - mse: 0.0282 - val_loss: 0.0429 - val_mae: 0.1505 - val_mse: 0.0429\n",
      "Epoch 3/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0235 - mae: 0.1087 - mse: 0.0235 - val_loss: 0.0248 - val_mae: 0.1096 - val_mse: 0.0248\n",
      "Epoch 4/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0092 - mae: 0.0670 - mse: 0.0092 - val_loss: 0.0276 - val_mae: 0.1141 - val_mse: 0.0276\n",
      "Epoch 5/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0070 - mae: 0.0582 - mse: 0.0070 - val_loss: 0.0227 - val_mae: 0.0997 - val_mse: 0.0227\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0264 - mae: 0.1036 - mse: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.09974052757024765\n",
      "R² Score: 0.7877680917399519\n",
      "Mean Squared Error: 0.022650307044386864\n",
      "Epoch 1/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.0058 - mae: 0.0499 - mse: 0.0056 - val_loss: 0.0261 - val_mae: 0.1041 - val_mse: 0.0261\n",
      "Epoch 2/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0048 - mae: 0.0454 - mse: 0.0047 - val_loss: 0.0265 - val_mae: 0.1048 - val_mse: 0.0265\n",
      "Epoch 3/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0045 - mae: 0.0442 - mse: 0.0043 - val_loss: 0.0274 - val_mae: 0.1025 - val_mse: 0.0274\n",
      "Epoch 4/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 0.0041 - mae: 0.0410 - mse: 0.0039 - val_loss: 0.0287 - val_mae: 0.1052 - val_mse: 0.0287\n",
      "Epoch 5/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0035 - mae: 0.0379 - mse: 0.0033 - val_loss: 0.0285 - val_mae: 0.1052 - val_mse: 0.0285\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0316 - mae: 0.1080 - mse: 0.0316\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.10522767901420593\n",
      "R² Score: 0.7328613399462671\n",
      "Mean Squared Error: 0.028510194271802902\n"
     ]
    }
   ],
   "source": [
    "run_block = True\n",
    "\n",
    "if run_block:\n",
    "    # Import necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "    # Normalizar la columna 'rating'\n",
    "    df['rating_normalized'] = df['rating'] / df['rating'].max()\n",
    "\n",
    "    # Prepare the text data\n",
    "    X = df['review'].values\n",
    "    y = df['rating_normalized'].values\n",
    "\n",
    "\n",
    "    # Split the dataset and also keep the indices for later use\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(X, y, df.index, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Pad the sequences\n",
    "    maxlen = 100\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    loss, mae, mse = model.evaluate(X_test_pad, y_test)\n",
    "\n",
    "    model.save('../../models/flybondi_review_predictor.h5')\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    # Use sample weights based on 'relevance_score'\n",
    "    sample_weights = df.loc[train_idx, 'relevance_score'].values\n",
    "    sample_weights = np.exp(sample_weights)\n",
    "    model.fit(X_train_pad, y_train, sample_weight=sample_weights, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "    # Evaluate the model again\n",
    "    loss, mae, mse = model.evaluate(X_test_pad, y_test)\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    model.save('../../models/flybondi_review_predictor_weighted.h5')\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Sentiment Analysis\n",
    "\n",
    "Sentiment analysis básico para ver cuales son positivas o negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "df['sentiment_score'] = df['review'].apply(get_sentiment_score)\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['sentiment_score'].apply(classify_sentiment)\n",
    "\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Configure the model and tokenizer with TensorFlow\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create the sentiment analysis pipeline with truncation enabled\n",
    "sentiment_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True  # Enable truncation here\n",
    ")\n",
    "\n",
    "# Function to get the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    result = sentiment_model(text)[0]\n",
    "    label = result['label']\n",
    "\n",
    "    # Convert the sentiment label to a composite score\n",
    "    if label == '1 star':\n",
    "        score = -1.0\n",
    "    elif label == '2 stars':\n",
    "        score = -0.5\n",
    "    elif label == '3 stars':\n",
    "        score = 0.0\n",
    "    elif label == '4 stars':\n",
    "        score = 0.5\n",
    "    elif label == '5 stars':\n",
    "        score = 1.0\n",
    "    return score\n",
    "\n",
    "# Apply the sentiment scoring function to each review\n",
    "df['sentiment_score_mbert'] = df['review'].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# VADER Sentiment Score Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['sentiment_score'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('VADER Sentiment Score Distribution')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# mBERT Sentiment Score Histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['sentiment_score_mbert'], bins=5, color='salmon', edgecolor='black')  # Bins correspond to possible scores\n",
    "plt.title('mBERT Sentiment Score Distribution')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# VADER Sentiment Score Density Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(df['sentiment_score'], shade=True, color='blue')\n",
    "plt.title('VADER Sentiment Score Density')\n",
    "plt.xlabel('Sentiment Score')\n",
    "\n",
    "# mBERT Sentiment Score Density Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(df['sentiment_score_mbert'], shade=True, color='red')\n",
    "plt.title('mBERT Sentiment Score Density')\n",
    "plt.xlabel('Sentiment Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df[['sentiment_score', 'sentiment_score_mbert']].corr(method='pearson')\n",
    "print(\"Correlation between VADER and mBERT sentiment scores:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['sentiment_score'], df['sentiment_score_mbert'], alpha=0.5)\n",
    "plt.title('Scatter Plot of VADER vs. mBERT Sentiment Scores')\n",
    "plt.xlabel('VADER Sentiment Score')\n",
    "plt.ylabel('mBERT Sentiment Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_mbert_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment_mbert'] = df['sentiment_score_mbert'].apply(classify_mbert_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = confusion_matrix(df['sentiment'], df['sentiment_mbert'], labels=['Positive', 'Neutral', 'Negative'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['Positive', 'Neutral', 'Negative'], columns=['Positive', 'Neutral', 'Negative'])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(df['sentiment'], df['sentiment_mbert'], labels=['Positive', 'Neutral', 'Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_df, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix Between VADER and mBERT Sentiments')\n",
    "plt.ylabel('VADER Sentiment')\n",
    "plt.xlabel('mBERT Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `df['sentiment_score']` and `df['sentiment_score_mbert']` already exist\n",
    "\n",
    "# Define a function to categorize VADER sentiment scores into 5 bins\n",
    "def categorize_vader_score(score):\n",
    "    if score <= -0.6:\n",
    "        return -1.0  # 1 star equivalent\n",
    "    elif score <= -0.2:\n",
    "        return -0.5  # 2 stars equivalent\n",
    "    elif score <= 0.2:\n",
    "        return 0.0   # 3 stars equivalent\n",
    "    elif score <= 0.6:\n",
    "        return 0.5   # 4 stars equivalent\n",
    "    else:\n",
    "        return 1.0   # 5 stars equivalent\n",
    "\n",
    "# Apply the categorization function to create comparable VADER scores\n",
    "df['vader_score_5bins'] = df['sentiment_score'].apply(categorize_vader_score)\n",
    "\n",
    "# Plotting the histograms for VADER (5 bins) and mBERT\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Adjusted VADER Sentiment Score Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['vader_score_5bins'], bins=5, color='skyblue', edgecolor='black')\n",
    "plt.title('VADER (Adjusted to 5 Levels) Sentiment Score Distribution')\n",
    "plt.xlabel('Sentiment Score (5 Levels)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# mBERT Sentiment Score Histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['sentiment_score_mbert'], bins=5, color='salmon', edgecolor='black')\n",
    "plt.title('mBERT Sentiment Score Distribution')\n",
    "plt.xlabel('Sentiment Score (5 Levels)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Subjectivity Analysis\n",
    "\n",
    "- 0= objetivo\n",
    "- 1= subjetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to get subjectivity scores\n",
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Apply the function to the dataset\n",
    "df['subjectivity'] = df['review'].apply(get_subjectivity)\n",
    "\n",
    "# Analyze the relationship between subjectivity and rating\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(df['subjectivity'], df['rating'])\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Subjectivity vs. Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot the distribution of subjectivity scores\n",
    "sns.histplot(df['subjectivity'], bins=20, kde=True)\n",
    "plt.xlabel('Subjectivity Score')\n",
    "plt.title('Distribution of Subjectivity Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of subjectivity scores by rating\n",
    "sns.boxplot(x='rating', y='subjectivity', data=df)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.title('Subjectivity Scores by Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "language_subjectivity = df.groupby('source_language')['subjectivity'].mean().reset_index()\n",
    "print(language_subjectivity)\n",
    "\n",
    "# Visualize\n",
    "sns.barplot(x='source_language', y='subjectivity', data=language_subjectivity)\n",
    "plt.xlabel('Source Language')\n",
    "plt.ylabel('Average Subjectivity')\n",
    "plt.title('Average Subjectivity by Language')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Topic Modeling with LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load English tokenizer, POS tagger, etc.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initial stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Domain-specific stopwords\n",
    "domain_stopwords = ['flight', 'airline', 'flybondi', 'plane', 'air', 'flights', 'fly', 'airlines']\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_.isalpha()]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['tokens'] = df['review'].apply(preprocess)\n",
    "\n",
    "# Most common tokens (to see if any should be added to stopwords)\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens)\n",
    "print(token_counts.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_threshold = 4\n",
    "bad_threshold = 2\n",
    "good_reviews = df[df['rating'] >= good_threshold].copy()\n",
    "bad_reviews = df[df['rating'] <= bad_threshold].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda(dataframe, num_topics=3):\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(dataframe['tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in dataframe['tokens']]\n",
    "\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        id2word=dictionary,\n",
    "        passes=10,\n",
    "        workers=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Print the topics\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topic {idx+1}: {topic}\\n\")\n",
    "\n",
    "    return lda_model, corpus, dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topics in Bad Reviews:\")\n",
    "bad_lda_model, bad_corpus, bad_dictionary = perform_lda(bad_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretación de 5 topics:\n",
    "- Topic 1; Schedule changes and delays affectinc travel plans\n",
    "- Topic 2: Poor customer service and unexpected charges\n",
    "- Topic 3: Flight cancellation and time wasted\n",
    "- Topic 4: Delays and financial loss\n",
    "- Topic 5: Payment issues and no reimbursments\n",
    "\n",
    "Interpretación de 3 topics:\n",
    "- Topic 1 : Schedule Changes and Delays\n",
    "- Topic 2 : Poor Service, Cancellations, and Extra Charges\n",
    "- Topic 3 : Flight Cancellations and Time Lost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topics in Good Reviews:\")\n",
    "good_lda_model, good_corpus, good_dictionary = perform_lda(good_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_word_cloud(lda_model, dictionary, topic_num):\n",
    "    words = dict(lda_model.show_topic(topic_num, 30))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic {topic_num+1}\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot word clouds for topics in bad reviews\n",
    "for topic_num in range(bad_lda_model.num_topics):\n",
    "    plot_word_cloud(bad_lda_model, bad_dictionary, topic_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English tokenizer, POS tagger, etc.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initial stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Domain-specific stopwords\n",
    "domain_stopwords = ['flight', 'airline', 'flybondi', 'plane', 'air', 'flights', 'fly', 'airlines']\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "df['tokens'] = df['review'].apply(preprocess)\n",
    "\n",
    "# Create dictionary and corpus from all reviews\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Train LDA model on all reviews\n",
    "num_topics = 5  # Adjust as needed\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    num_topics=num_topics,\n",
    "    id2word=dictionary,\n",
    "    passes=10,\n",
    "    workers=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\\n\")\n",
    "\n",
    "# Get topic distributions for each review\n",
    "def get_topic_distribution(lda_model, corpus):\n",
    "    return [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
    "\n",
    "df['topic_distribution'] = get_topic_distribution(lda_model, corpus)\n",
    "\n",
    "# Label reviews\n",
    "good_threshold = 4\n",
    "bad_threshold = 2\n",
    "df['label'] = df['rating'].apply(lambda x: 'good' if x >= good_threshold else ('bad' if x <= bad_threshold else 'neutral'))\n",
    "\n",
    "# Separate reviews\n",
    "good_reviews = df[df['label'] == 'good']\n",
    "bad_reviews = df[df['label'] == 'bad']\n",
    "\n",
    "# Calculate average topic distributions\n",
    "def average_topic_distribution(reviews):\n",
    "    distributions = []\n",
    "    for dist in reviews['topic_distribution']:\n",
    "        dist_dict = {topic_id: prob for topic_id, prob in dist}\n",
    "        distributions.append(dist_dict)\n",
    "    df_distributions = pd.DataFrame(distributions).fillna(0)\n",
    "    return df_distributions.mean()\n",
    "\n",
    "good_topic_avg = average_topic_distribution(good_reviews)\n",
    "bad_topic_avg = average_topic_distribution(bad_reviews)\n",
    "\n",
    "# Plot average topic distributions\n",
    "topics = [f'Topic {i}' for i in range(num_topics)]\n",
    "x = np.arange(len(topics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, good_topic_avg, width, label='Good Reviews')\n",
    "rects2 = ax.bar(x + width/2, bad_topic_avg, width, label='Bad Reviews')\n",
    "\n",
    "ax.set_ylabel('Average Topic Proportion')\n",
    "ax.set_title('Average Topic Distribution by Review Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(topics)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot word clouds for all topics\n",
    "def plot_word_cloud(lda_model, topic_num):\n",
    "    words = dict(lda_model.show_topic(topic_num, 30))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Topic {topic_num}\")\n",
    "    plt.show()\n",
    "\n",
    "for topic_num in range(num_topics):\n",
    "    plot_word_cloud(lda_model, topic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Aspect-Based Sentiment analysis\n",
    "\n",
    "- provee información sobre areas específicas de mejora\n",
    "- ayuda a priorizar acciones basadas en aspectos que se consideraron negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "aspects = ['service', 'price', 'comfort', 'staff', 'food', 'flight', 'delay']\n",
    "\n",
    "def aspect_sentiment(review):\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    aspect_sentiments = {}\n",
    "    for aspect in aspects:\n",
    "        aspect_sentiments[aspect] = []\n",
    "        for sentence in sentences:\n",
    "            if aspect in sentence.lower():\n",
    "                score = sid.polarity_scores(sentence)['compound']\n",
    "                aspect_sentiments[aspect].append(score)\n",
    "    # Average sentiment per aspect\n",
    "    for aspect in aspect_sentiments:\n",
    "        if aspect_sentiments[aspect]:\n",
    "            aspect_sentiments[aspect] = np.mean(aspect_sentiments[aspect])\n",
    "        else:\n",
    "            aspect_sentiments[aspect] = None\n",
    "    return aspect_sentiments\n",
    "\n",
    "df['aspect_sentiments'] = df['review'].apply(aspect_sentiment)\n",
    "\n",
    "aspect_df = df['aspect_sentiments'].apply(pd.Series)\n",
    "\n",
    "aspect_summary = aspect_df.mean()\n",
    "print(aspect_summary)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(aspect_summary.index, aspect_summary.values, color='skyblue')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.title('Average Sentiment Score by Aspect')\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Flight Experience Stages\n",
    "- Se puede sacar el relevance_score si se quiere uno promediado de manera uniforme\n",
    "- ayuda a encontrar areas de mejora particulares a un proceso de vuelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Palabras asociadas a flight stages\n",
    "flight_stages_keywords = {\n",
    "    'booking': ['booking', 'reservation', 'ticket purchase', 'purchase', 'buying tickets', 'online booking'],\n",
    "    'check-in': ['check-in', 'checkin', 'check in', 'baggage drop', 'luggage drop', 'counter', 'kiosk', 'online check-in'],\n",
    "    'boarding': ['boarding', 'gate', 'queue', 'line', 'boarding process', 'security', 'passport control'],\n",
    "    'in-flight': ['in-flight', 'flight', 'onboard', 'cabin', 'crew', 'seat', 'seating', 'entertainment', 'food', 'beverage', 'service', 'comfort', 'turbulence'],\n",
    "    'post-flight': ['post-flight', 'arrival', 'baggage claim', 'lost luggage', 'customer service', 'complaint', 'feedback', 'delay', 'connections', 'transit']\n",
    "}\n",
    "\n",
    "# Como el valor de relevance es muy chico entonces lo scaleamos un poco\n",
    "df['scaled_relevance'] = df['relevance_score'] * 10\n",
    "\n",
    "def get_stage_sentiments_weighted(row):\n",
    "    review = row['review']\n",
    "    sentiment_score = row['sentiment_score']\n",
    "    scaled_relevance = row['scaled_relevance']\n",
    "    stage_sentiments = {stage: [] for stage in flight_stages_keywords.keys()}\n",
    "    review_lower = review.lower()\n",
    "    # por cada review se busca si hay alguna palabra clave en cada etapa\n",
    "    for stage, keywords in flight_stages_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', review_lower):\n",
    "                # si se encuentra la palabra clave se ajusta el score\n",
    "                adjusted_score = sentiment_score * scaled_relevance # si no queremos que se ajuste en base a relavance sacamos el scaled_relevance\n",
    "                stage_sentiments[stage].append(adjusted_score)\n",
    "\n",
    "    # se hace una pasada por cada etapa para calcular el promedio de los scores ( sum(score * relevance) / sum(relevance) )\n",
    "    for stage in stage_sentiments:\n",
    "        if stage_sentiments[stage]:\n",
    "            stage_sentiments[stage] = np.mean(stage_sentiments[stage])\n",
    "        else:\n",
    "            stage_sentiments[stage] = np.nan\n",
    "    return pd.Series(stage_sentiments)\n",
    "\n",
    "df_stage_sentiments = df.apply(get_stage_sentiments_weighted, axis=1)\n",
    "\n",
    "# --- Graph 1: Combined Heatmap ---\n",
    "\n",
    "# Calculate average sentiment score for each stage\n",
    "stage_avg_sentiments = df_stage_sentiments.mean()\n",
    "\n",
    "# Plot combined heatmap\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(stage_avg_sentiments.values.reshape(1, -1), annot=True, fmt=\".2f\", cmap='magma', xticklabels=stage_avg_sentiments.index)\n",
    "plt.title('Average Sentiment Score per Flight Stage (Weighted by Relevance Score)')\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# --- Graph 2: Heatmap by Rating ---\n",
    "\n",
    "# Combine stage sentiments with ratings\n",
    "df_stage_ratings = pd.concat([df[['rating']], df_stage_sentiments], axis=1)\n",
    "\n",
    "# Group by rating and calculate mean sentiment scores for each stage\n",
    "stage_rating_avg_sentiments = df_stage_ratings.groupby('rating').mean()\n",
    "\n",
    "# Order the ratings from 1 to 5\n",
    "stage_rating_avg_sentiments = stage_rating_avg_sentiments.sort_index(ascending=True)\n",
    "\n",
    "# Plot heatmap separated by rating score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(stage_rating_avg_sentiments, annot=True, fmt=\".2f\", cmap='magma', xticklabels=stage_rating_avg_sentiments.columns, yticklabels=stage_rating_avg_sentiments.index)\n",
    "plt.title('Average Sentiment Score per Flight Stage by Rating (Weighted by Relevance Score)')\n",
    "plt.xlabel('Flight Stage')\n",
    "plt.ylabel('Rating')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# --- Graph 3: Bar Plot of Average Adjusted Sentiment Score per Rating ---\n",
    "\n",
    "# Calculate adjusted sentiment score for each review\n",
    "df['adjusted_sentiment'] = df['sentiment_score'] * df['scaled_relevance']\n",
    "\n",
    "# Group by rating and calculate mean adjusted sentiment\n",
    "rating_avg_sentiment = df.groupby('rating')['adjusted_sentiment'].mean().reset_index()\n",
    "\n",
    "# Order the ratings from 1 to 5\n",
    "rating_avg_sentiment = rating_avg_sentiment.sort_values('rating')\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='rating', y='adjusted_sentiment', data=rating_avg_sentiment, order=rating_avg_sentiment['rating'])\n",
    "plt.title('Average Adjusted Sentiment Score per Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Average Adjusted Sentiment Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# --- Graph 4: Line Plot of Sentiment Trend Across Flight Stages ---\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "stage_avg_sentiments.plot(kind='line', marker='o')\n",
    "plt.title('Sentiment Trend Across Flight Stages (Weighted by Relevance Score)')\n",
    "plt.xlabel('Flight Stage')\n",
    "plt.ylabel('Average Adjusted Sentiment Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Net Promoter Score (Based on Vader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NPS categories based on sentiment score\n",
    "def predict_nps(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Promoter'\n",
    "    elif score <= -0.05:\n",
    "        return 'Detractor'\n",
    "    else:\n",
    "        return 'Passive'\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "df['nps_category'] = df['sentiment_score'].apply(predict_nps)\n",
    "\n",
    "# Check the distribution of NPS categories\n",
    "print(df['nps_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Review summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join reviews and ask for a summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
