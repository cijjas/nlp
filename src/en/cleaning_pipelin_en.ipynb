{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pipeline\n",
    "\n",
    "First we load the recollected data in this case from [this file](../../data/final_combined_reviews.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/final_combined_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Process\n",
    "\n",
    "We remove unused collumns (can be upscaled to consider more cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnamed columns which are trash\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We normalize the domain of our ratings to be integers and lower the case for the names to be used as unique identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['rating_cleaned'] = df_cleaned['rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "df_cleaned['rating_cleaned'] = df_cleaned['rating_cleaned'].fillna(1).astype(int)\n",
    "\n",
    "df_cleaned = df_cleaned.drop(columns=['rating'])\n",
    "df_cleaned = df_cleaned.rename(columns={'rating_cleaned': 'rating'})\n",
    "\n",
    "df_cleaned['name'] = df_cleaned['name'].str.lower()\n",
    "\n",
    "df_cleaned.iloc[1300:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reviews have titles, in this pipeline we concatenate them so that they can be considered part of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['review'] = df_cleaned['review_title'].fillna('') + \\\n",
    "                       df_cleaned['review_title'].apply(lambda x: '. ' if pd.notna(x) and x != '' else '') + \\\n",
    "                       df_cleaned['review_text'].fillna('')\n",
    "\n",
    "df_cleaned = df_cleaned.drop(columns=['review_title', 'review_text'])\n",
    "\n",
    "df_cleaned['review'] = df_cleaned['review'].str.lower()\n",
    "\n",
    "df_cleaned.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reviews might be repeated due to scraping inconsistencies but can be easily removed by comparing the lowercase names, so we remove duplicates (comparing names keeping the longest review since some names can give multiple reviews and we just want one so there is no bias implication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def longest_review(group):\n",
    "    return group.loc[group['review'].str.len().idxmax()]\n",
    "\n",
    "df_cleaned = df_cleaned.groupby('name', group_keys=False).apply(longest_review)\n",
    "\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We format likes to be integers o 0 if no likes were given to the specific review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['likes'] = df_cleaned['likes'].fillna(0)\n",
    "df_cleaned['likes'] = df_cleaned['likes'].astype(int)\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create a \"relevance score\" which will be used to allow weighting of reviews importance since some reviewer can be considered more important than others in terms of reviewing experience. This is a normalized score ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_experience(experience):\n",
    "    resenas = 0\n",
    "    fotos = 0\n",
    "    local_guide = 0\n",
    "\n",
    "    if pd.isna(experience):\n",
    "        return resenas, fotos, local_guide\n",
    "\n",
    "    if 'Local Guide' in experience:\n",
    "        local_guide = 1\n",
    "\n",
    "    resenas_match = re.search(r'(\\d+[\\.,]?\\d*) (reseñas|opinión|opiniones)', experience)\n",
    "    if resenas_match:\n",
    "        resenas = int(resenas_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    fotos_match = re.search(r'(\\d+[\\.,]?\\d*) fotos', experience)\n",
    "    if fotos_match:\n",
    "        fotos = int(fotos_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    return resenas, fotos, local_guide\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']] = df_cleaned['experience'].apply(\n",
    "    lambda x: pd.Series(parse_experience(x))\n",
    ")\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']]\n",
    "df_cleaned = df_cleaned.drop(columns=['experience'])\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(row, W_l=0.3, W_r=0.5, W_p=0.005, W_lg=0.5):\n",
    "    relevance = (\n",
    "        W_l * row['likes'] +\n",
    "        W_r * row['given_reviews'] +\n",
    "        W_p * row['pictures'] +\n",
    "        W_lg * row['local_guide']\n",
    "    )\n",
    "    return relevance\n",
    "\n",
    "df_cleaned['relevance_score'] = df_cleaned.apply(calculate_relevance, axis=1)\n",
    "df_cleaned['relevance_score_normalized'] = (df_cleaned['relevance_score'] - df_cleaned['relevance_score'].min()) / (df_cleaned['relevance_score'].max() - df_cleaned['relevance_score'].min())\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the languages of each review so that they can be later translated to english using googletranslate api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)  # Returns a language code (e.g., 'en', 'es')\n",
    "    except LangDetectException:\n",
    "        return 'unknown'  # Handle cases where language detection fails\n",
    "\n",
    "df_cleaned['language'] = df_cleaned['review'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame with the new 'language' column\n",
    "df_cleaned[['review', 'language']]\n",
    "\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/before_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate each review to english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_spanish(text, src_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(\"could not translate: \", text)\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text\n",
    "\n",
    "def translate_non_spanish(text, lang):\n",
    "    if lang != 'en' and lang != 'unknown':\n",
    "        return translate_to_spanish(text, lang)\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned.apply(\n",
    "    lambda row: translate_non_spanish(row['review'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_cleaned[['review', 'language', 'review_translated']]\n",
    "\n",
    "# we save the df just in case we want to check the translations or the original reviews\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize as part of a fun experiment the distribution of languages in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Use saved dataframe\n",
    "# df_cleaned = pd.read_csv('your_file.csv')  \n",
    "\n",
    "language_counts = df_cleaned['source_language'].value_counts()\n",
    "\n",
    "top_languages = language_counts[:3]  # Top 4 languages\n",
    "others_count = language_counts[3:].sum()  # Suma del resto de los idiomas\n",
    "others_series = pd.Series([others_count], index=['Others'])\n",
    "\n",
    "language_counts_modified = pd.concat([top_languages, others_series])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(language_counts_modified, labels=language_counts_modified.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Source Languages')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 \n",
    "\n",
    "We consider this a relevant checkpoint since this part takes a bit of time to execute. Here we use stanza lemmatizer to get a brief relevant and neutralized review without stopwords emojis or whatever that will not be further analyzed. So first we remove these parts which are not of importance to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "flybondi_data = '../../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned = pd.read_csv(flybondi_data)\n",
    "\n",
    "df_cleaned = df_cleaned[~(df_cleaned['review_translated'].isna() | (df_cleaned['review_translated'].astype(str).str.strip() == ''))]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = string.punctuation + '¡'\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  #  transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  #  Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \" \", text)\n",
    "\n",
    "    text_words = text.split()\n",
    "    text = ' '.join([word for word in text_words if word not in english_stopwords])\n",
    "\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned['review_translated'].astype(str)\n",
    "\n",
    "df_cleaned['review_processed'] = df_cleaned['review_translated'].apply(preprocess_text)\n",
    "\n",
    "df_cleaned[['review_translated', 'review_processed']]\n",
    "\n",
    "df_cleaned = df_cleaned.rename(columns={'language': 'source_language'})\n",
    "df_cleaned = df_cleaned.drop(columns=['review_translated', 'relevance_score', 'review', 'given_reviews', 'pictures', 'local_guide', 'likes'])\n",
    "\n",
    "df_cleaned = df_cleaned.rename(columns={'review_processed': 'review'})\n",
    "df_cleaned = df_cleaned.rename(columns={'relevance_score_normalized': 'relevance_score'})\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def lemmatize_english(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "df['review'] = df['review'].apply(lemmatize_english)\n",
    "\n",
    "output_file_lemmatized_reviews = '../../data/en_cleaned_with_lemmatized_reviews.csv'\n",
    "df.to_csv(output_file_lemmatized_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
