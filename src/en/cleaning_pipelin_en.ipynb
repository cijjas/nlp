{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo de reviews scrapeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/final_combined_reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m flybondi_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/final_combined_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflybondi_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/final_combined_reviews.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/final_combined_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a limpiar.\n",
    "\n",
    "1. Remover columnas inútiles y filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnamed columns which are trash\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos ratings para que tenga unico formato (int), pasamos nombres a lower para id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo los números de la columna 'rating' y convertir a float\n",
    "df_cleaned['rating_cleaned'] = df_cleaned['rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "df_cleaned['rating_cleaned'] = df_cleaned['rating_cleaned'].fillna(1).astype(int)\n",
    "\n",
    "# Eliminar la columna original 'rating' y renombrar la nueva columna a 'rating'\n",
    "df_cleaned = df_cleaned.drop(columns=['rating'])\n",
    "df_cleaned = df_cleaned.rename(columns={'rating_cleaned': 'rating'})\n",
    "\n",
    "# Convertir a minúsculas la columna 'name'\n",
    "df_cleaned['name'] = df_cleaned['name'].str.lower()\n",
    "\n",
    "# Mostrar las filas a partir de la 1300\n",
    "df_cleaned.iloc[1300:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos titulos con reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar solo si 'review_title' no está vacío\n",
    "df_cleaned['review'] = df_cleaned['review_title'].fillna('') + \\\n",
    "                       df_cleaned['review_title'].apply(lambda x: '. ' if pd.notna(x) and x != '' else '') + \\\n",
    "                       df_cleaned['review_text'].fillna('')\n",
    "\n",
    "# Eliminar las columnas 'review_title' y 'review_text'\n",
    "df_cleaned = df_cleaned.drop(columns=['review_title', 'review_text'])\n",
    "\n",
    "# Convertir todo a minúsculas\n",
    "df_cleaned['review'] = df_cleaned['review'].str.lower()\n",
    "\n",
    "# Mostrar las últimas filas\n",
    "df_cleaned.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos reviews que tengan nombre repetido dejando el review más largo ya que esto fue error de scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Función para obtener la fila con la reseña más larga dentro de cada grupo\n",
    "def longest_review(group):\n",
    "    # Asegurarte de que 'name' esté en el resultado\n",
    "    return group.loc[group['review'].str.len().idxmax()]\n",
    "# Aplicar la función longest_review al grupo, sin que 'name' desaparezca\n",
    "df_cleaned = df_cleaned.groupby('name', group_keys=False).apply(longest_review)\n",
    "\n",
    "# Restablecer el índice para tener el DataFrame limpio\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verificar que 'name' esté presente\n",
    "df_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formateamos \"likes\" para que sea int y los que sean Nan sean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['likes'] = df_cleaned['likes'].fillna(0)\n",
    "df_cleaned['likes'] = df_cleaned['likes'].astype(int)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que transforma la experience y los likes columnas para transformar después en un único valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_experience(experience):\n",
    "    resenas = 0\n",
    "    fotos = 0\n",
    "    local_guide = 0\n",
    "\n",
    "    if pd.isna(experience):\n",
    "        return resenas, fotos, local_guide\n",
    "\n",
    "    if 'Local Guide' in experience:\n",
    "        local_guide = 1\n",
    "\n",
    "    resenas_match = re.search(r'(\\d+[\\.,]?\\d*) (reseñas|opinión|opiniones)', experience)\n",
    "    if resenas_match:\n",
    "        resenas = int(resenas_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    fotos_match = re.search(r'(\\d+[\\.,]?\\d*) fotos', experience)\n",
    "    if fotos_match:\n",
    "        fotos = int(fotos_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    return resenas, fotos, local_guide\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']] = df_cleaned['experience'].apply(\n",
    "    lambda x: pd.Series(parse_experience(x))\n",
    ")\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']]\n",
    "df_cleaned = df_cleaned.drop(columns=['experience'])\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(row, W_l=0.3, W_r=0.5, W_p=0.005, W_lg=0.5):\n",
    "    relevance = (\n",
    "        W_l * row['likes'] +\n",
    "        W_r * row['given_reviews'] +\n",
    "        W_p * row['pictures'] +\n",
    "        W_lg * row['local_guide']\n",
    "    )\n",
    "    return relevance\n",
    "\n",
    "df_cleaned['relevance_score'] = df_cleaned.apply(calculate_relevance, axis=1)\n",
    "df_cleaned['relevance_score_normalized'] = (df_cleaned['relevance_score'] - df_cleaned['relevance_score'].min()) / (df_cleaned['relevance_score'].max() - df_cleaned['relevance_score'].min())\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)  # Returns a language code (e.g., 'en', 'es')\n",
    "    except LangDetectException:\n",
    "        return 'unknown'  # Handle cases where language detection fails\n",
    "\n",
    "df_cleaned['language'] = df_cleaned['review'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame with the new 'language' column\n",
    "df_cleaned[['review', 'language']]\n",
    "\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/before_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que porcentajes de reviews hay escritos en cada idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_spanish(text, src_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(\"could not translate: \", text)\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text\n",
    "\n",
    "def translate_non_spanish(text, lang):\n",
    "    if lang != 'en' and lang != 'unknown':\n",
    "        return translate_to_spanish(text, lang)\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned.apply(\n",
    "    lambda row: translate_non_spanish(row['review'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_cleaned[['review', 'language', 'review_translated']]\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Simular el DataFrame con datos de ejemplo\n",
    "# df_cleaned = pd.read_csv('your_file.csv')  # Si ya tienes el archivo cargado\n",
    "\n",
    "# Contar la ocurrencia de cada idioma\n",
    "language_counts = df_cleaned['source_language'].value_counts()\n",
    "\n",
    "# Separar los 4 idiomas más comunes y agrupar el resto en \"Others\"\n",
    "top_languages = language_counts[:3]  # Top 4 languages\n",
    "others_count = language_counts[3:].sum()  # Suma del resto de los idiomas\n",
    "others_series = pd.Series([others_count], index=['Others'])\n",
    "\n",
    "# Usar pd.concat en lugar de append\n",
    "language_counts_modified = pd.concat([top_languages, others_series])\n",
    "\n",
    "# Crear el gráfico de torta\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(language_counts_modified, labels=language_counts_modified.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Source Languages')\n",
    "plt.axis('equal')  # Mantener aspecto igual para que la torta sea circular\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 \n",
    "\n",
    "desde acá se puede ejecutar sin ejecutar lo previo si se cuenta con un archivo traducido en cleaning_pipeline. Es posible que sea necesario volver a ejecutar el import de algunas librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jgirod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>source_language</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>es</td>\n",
       "      <td>terrible terrible worst vuando lost hotel day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>one worst flown flown hundreds airlines possib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>terrible service flight first delayed 25 min s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>rubbish low cost airline bought 6 tickets via ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885david_r885</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>refunding cancelled flights refunding canceled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>валерия шульга</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>es</td>\n",
       "      <td>flight reprogrammed without informing us paid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>вика мегалис</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>es</td>\n",
       "      <td>would like tell everyone never buy flybondi fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>дарья венедиктова</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>es</td>\n",
       "      <td>company come year ago waste time coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>יסמין י</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>dont buy scam completely ridiculous flight del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>최승현</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>es</td>\n",
       "      <td>never never never use airline going check alre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1849 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  rating  relevance_score source_language  \\\n",
       "0           22fortinero       1         0.000000              es   \n",
       "1            23russellv       2         0.000000              en   \n",
       "2               4family       1         0.000000              en   \n",
       "3     5travellers602013       1         0.000000              en   \n",
       "4         885david_r885       1         0.000000              en   \n",
       "...                 ...     ...              ...             ...   \n",
       "1888     валерия шульга       1         0.001745              es   \n",
       "1889       вика мегалис       1         0.004072              es   \n",
       "1890  дарья венедиктова       1         0.002909              es   \n",
       "1891            יסמין י       1         0.000000              en   \n",
       "1892                최승현       1         0.004479              es   \n",
       "\n",
       "                                                 review  \n",
       "0     terrible terrible worst vuando lost hotel day ...  \n",
       "1     one worst flown flown hundreds airlines possib...  \n",
       "2     terrible service flight first delayed 25 min s...  \n",
       "3     rubbish low cost airline bought 6 tickets via ...  \n",
       "4     refunding cancelled flights refunding canceled...  \n",
       "...                                                 ...  \n",
       "1888  flight reprogrammed without informing us paid ...  \n",
       "1889  would like tell everyone never buy flybondi fl...  \n",
       "1890            company come year ago waste time coming  \n",
       "1891  dont buy scam completely ridiculous flight del...  \n",
       "1892  never never never use airline going check alre...  \n",
       "\n",
       "[1849 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Cargar los datos\n",
    "flybondi_data = '../../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned = pd.read_csv(flybondi_data)\n",
    "\n",
    "# Filtrar las filas donde 'review_translated' es nulo o está vacío\n",
    "df_cleaned = df_cleaned[~(df_cleaned['review_translated'].isna() | (df_cleaned['review_translated'].astype(str).str.strip() == ''))]\n",
    "\n",
    "# Cambiar a stopwords en inglés\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Definir la puntuación, pero en vez de eliminarla, la reemplazamos con un espacio\n",
    "punctuation = string.punctuation + '¡'\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Expresión regular para detectar emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticones\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # símbolos de transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"  # caracteres cerrados\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Asegurar que el texto no sea nulo o NaN\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Reemplazar puntuación con espacios en lugar de simplemente eliminarla\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \" \", text)\n",
    "\n",
    "    # Eliminar stopwords en inglés\n",
    "    text_words = text.split()\n",
    "    text = ' '.join([word for word in text_words if word not in english_stopwords])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Convertir todos los valores en 'review_translated' a string, y manejar NaN\n",
    "df_cleaned['review_translated'] = df_cleaned['review_translated'].astype(str)\n",
    "\n",
    "# Aplicar el preprocesamiento a la columna 'review_translated'\n",
    "df_cleaned['review_processed'] = df_cleaned['review_translated'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar las columnas procesadas\n",
    "df_cleaned[['review_translated', 'review_processed']]\n",
    "\n",
    "# Eliminar la columna 'review_translated'\n",
    "df_cleaned = df_cleaned.rename(columns={'language': 'source_language'})\n",
    "df_cleaned = df_cleaned.drop(columns=['review_translated', 'relevance_score', 'review', 'given_reviews', 'pictures', 'local_guide', 'likes'])\n",
    "\n",
    "# Renombrar 'review_processed' a 'review'\n",
    "df_cleaned = df_cleaned.rename(columns={'review_processed': 'review'})\n",
    "df_cleaned = df_cleaned.rename(columns={'relevance_score_normalized': 'relevance_score'})\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizador\n",
    "Ahora que tenemos un dataset con todas las reviews limpias y traducidas procedemos a lemmatizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgirod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 45.4MB/s]                    \n",
      "2024-10-20 20:51:41 INFO: Downloaded file to /home/jgirod/stanza_resources/resources.json\n",
      "2024-10-20 20:51:41 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-20 20:51:42 INFO: File exists: /home/jgirod/stanza_resources/en/default.zip\n",
      "2024-10-20 20:51:45 INFO: Finished downloading models and saved to /home/jgirod/stanza_resources\n",
      "2024-10-20 20:51:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 30.7MB/s]                    \n",
      "2024-10-20 20:51:45 INFO: Downloaded file to /home/jgirod/stanza_resources/resources.json\n",
      "2024-10-20 20:51:46 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-20 20:51:46 INFO: Using device: cpu\n",
      "2024-10-20 20:51:46 INFO: Loading: tokenize\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:46 INFO: Loading: mwt\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:46 INFO: Loading: pos\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: lemma\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: constituency\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: depparse\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: sentiment\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: ner\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "# Descargar el modelo de inglés\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def lemmatize_english(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "\n",
    "# Aplicar la función de lematización a las reviews en inglés\n",
    "df['review'] = df['review'].apply(lemmatize_english)\n",
    "\n",
    "# Guardar el DataFrame actualizado con las reviews lematizadas\n",
    "output_file_lemmatized_reviews = '../../data/en_cleaned_with_lemmatized_reviews.csv'\n",
    "df.to_csv(output_file_lemmatized_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
