{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo de reviews scrapeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/final_combined_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a limpiar.\n",
    "\n",
    "1. Remover columnas inútiles y filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnamed columns which are trash\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos ratings para que tenga unico formato (int), pasamos nombres a lower para id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo los números de la columna 'rating' y convertir a float\n",
    "df_cleaned['rating_cleaned'] = df_cleaned['rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "df_cleaned['rating_cleaned'] = df_cleaned['rating_cleaned'].fillna(1).astype(int)\n",
    "\n",
    "# Eliminar la columna original 'rating' y renombrar la nueva columna a 'rating'\n",
    "df_cleaned = df_cleaned.drop(columns=['rating'])\n",
    "df_cleaned = df_cleaned.rename(columns={'rating_cleaned': 'rating'})\n",
    "\n",
    "# Convertir a minúsculas la columna 'name'\n",
    "df_cleaned['name'] = df_cleaned['name'].str.lower()\n",
    "\n",
    "# Mostrar las filas a partir de la 1300\n",
    "df_cleaned.iloc[1300:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos titulos con reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar solo si 'review_title' no está vacío\n",
    "df_cleaned['review'] = df_cleaned['review_title'].fillna('') + \\\n",
    "                       df_cleaned['review_title'].apply(lambda x: '. ' if pd.notna(x) and x != '' else '') + \\\n",
    "                       df_cleaned['review_text'].fillna('')\n",
    "\n",
    "# Eliminar las columnas 'review_title' y 'review_text'\n",
    "df_cleaned = df_cleaned.drop(columns=['review_title', 'review_text'])\n",
    "\n",
    "# Convertir todo a minúsculas\n",
    "df_cleaned['review'] = df_cleaned['review'].str.lower()\n",
    "\n",
    "# Mostrar las últimas filas\n",
    "df_cleaned.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos reviews que tengan nombre repetido dejando el review más largo ya que esto fue error de scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Función para obtener la fila con la reseña más larga dentro de cada grupo\n",
    "def longest_review(group):\n",
    "    # Asegurarte de que 'name' esté en el resultado\n",
    "    return group.loc[group['review'].str.len().idxmax()]\n",
    "# Aplicar la función longest_review al grupo, sin que 'name' desaparezca\n",
    "df_cleaned = df_cleaned.groupby('name', group_keys=False).apply(longest_review)\n",
    "\n",
    "# Restablecer el índice para tener el DataFrame limpio\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verificar que 'name' esté presente\n",
    "df_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formateamos \"likes\" para que sea int y los que sean Nan sean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['likes'] = df_cleaned['likes'].fillna(0)\n",
    "df_cleaned['likes'] = df_cleaned['likes'].astype(int)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que transforma la experience y los likes columnas para transformar después en un único valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_experience(experience):\n",
    "    resenas = 0\n",
    "    fotos = 0\n",
    "    local_guide = 0\n",
    "\n",
    "    if pd.isna(experience):\n",
    "        return resenas, fotos, local_guide\n",
    "\n",
    "    if 'Local Guide' in experience:\n",
    "        local_guide = 1\n",
    "\n",
    "    resenas_match = re.search(r'(\\d+[\\.,]?\\d*) (reseñas|opinión|opiniones)', experience)\n",
    "    if resenas_match:\n",
    "        resenas = int(resenas_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    fotos_match = re.search(r'(\\d+[\\.,]?\\d*) fotos', experience)\n",
    "    if fotos_match:\n",
    "        fotos = int(fotos_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    return resenas, fotos, local_guide\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']] = df_cleaned['experience'].apply(\n",
    "    lambda x: pd.Series(parse_experience(x))\n",
    ")\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']]\n",
    "df_cleaned = df_cleaned.drop(columns=['experience'])\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(row, W_l=0.3, W_r=0.5, W_p=0.005, W_lg=0.5):\n",
    "    relevance = (\n",
    "        W_l * row['likes'] +\n",
    "        W_r * row['given_reviews'] +\n",
    "        W_p * row['pictures'] +\n",
    "        W_lg * row['local_guide']\n",
    "    )\n",
    "    return relevance\n",
    "\n",
    "df_cleaned['relevance_score'] = df_cleaned.apply(calculate_relevance, axis=1)\n",
    "df_cleaned['relevance_score_normalized'] = (df_cleaned['relevance_score'] - df_cleaned['relevance_score'].min()) / (df_cleaned['relevance_score'].max() - df_cleaned['relevance_score'].min())\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)  # Returns a language code (e.g., 'en', 'es')\n",
    "    except LangDetectException:\n",
    "        return 'unknown'  # Handle cases where language detection fails\n",
    "\n",
    "df_cleaned['language'] = df_cleaned['review'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame with the new 'language' column\n",
    "df_cleaned[['review', 'language']]\n",
    "\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/before_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que porcentajes de reviews hay escritos en cada idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_spanish(text, src_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(\"could not translate: \", text)\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text\n",
    "\n",
    "def translate_non_spanish(text, lang):\n",
    "    if lang != 'en' and lang != 'unknown':\n",
    "        return translate_to_spanish(text, lang)\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned.apply(\n",
    "    lambda row: translate_non_spanish(row['review'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_cleaned[['review', 'language', 'review_translated']]\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "output_file_translated_reviews = '../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Simular el DataFrame con datos de ejemplo\n",
    "# df_cleaned = pd.read_csv('your_file.csv')  # Si ya tienes el archivo cargado\n",
    "\n",
    "# Contar la ocurrencia de cada idioma\n",
    "language_counts = df_cleaned['source_language'].value_counts()\n",
    "\n",
    "# Separar los 4 idiomas más comunes y agrupar el resto en \"Others\"\n",
    "top_languages = language_counts[:3]  # Top 4 languages\n",
    "others_count = language_counts[3:].sum()  # Suma del resto de los idiomas\n",
    "others_series = pd.Series([others_count], index=['Others'])\n",
    "\n",
    "# Usar pd.concat en lugar de append\n",
    "language_counts_modified = pd.concat([top_languages, others_series])\n",
    "\n",
    "# Crear el gráfico de torta\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(language_counts_modified, labels=language_counts_modified.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Source Languages')\n",
    "plt.axis('equal')  # Mantener aspecto igual para que la torta sea circular\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 \n",
    "\n",
    "desde acá se puede ejecutar sin ejecutar lo previo si se cuenta con un archivo traducido en cleaning_pipeline. Es posible que sea necesario volver a ejecutar el import de algunas librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jgirod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>source_language</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>es</td>\n",
       "      <td>terrible terrible worst vuando lost hotel day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>one worst flown flown hundreds airlines possib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>terrible service flight first delayed 25 min s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>rubbish low cost airline bought 6 tickets via ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885david_r885</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>refunding cancelled flights refunding canceled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>валерия шульга</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>es</td>\n",
       "      <td>flight reprogrammed without informing us paid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>вика мегалис</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>es</td>\n",
       "      <td>would like tell everyone never buy flybondi fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>дарья венедиктова</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>es</td>\n",
       "      <td>company come year ago waste time coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>יסמין י</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>dont buy scam completely ridiculous flight del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>최승현</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>es</td>\n",
       "      <td>never never never use airline going check alre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1849 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  rating  relevance_score source_language  \\\n",
       "0           22fortinero       1         0.000000              es   \n",
       "1            23russellv       2         0.000000              en   \n",
       "2               4family       1         0.000000              en   \n",
       "3     5travellers602013       1         0.000000              en   \n",
       "4         885david_r885       1         0.000000              en   \n",
       "...                 ...     ...              ...             ...   \n",
       "1888     валерия шульга       1         0.001745              es   \n",
       "1889       вика мегалис       1         0.004072              es   \n",
       "1890  дарья венедиктова       1         0.002909              es   \n",
       "1891            יסמין י       1         0.000000              en   \n",
       "1892                최승현       1         0.004479              es   \n",
       "\n",
       "                                                 review  \n",
       "0     terrible terrible worst vuando lost hotel day ...  \n",
       "1     one worst flown flown hundreds airlines possib...  \n",
       "2     terrible service flight first delayed 25 min s...  \n",
       "3     rubbish low cost airline bought 6 tickets via ...  \n",
       "4     refunding cancelled flights refunding canceled...  \n",
       "...                                                 ...  \n",
       "1888  flight reprogrammed without informing us paid ...  \n",
       "1889  would like tell everyone never buy flybondi fl...  \n",
       "1890            company come year ago waste time coming  \n",
       "1891  dont buy scam completely ridiculous flight del...  \n",
       "1892  never never never use airline going check alre...  \n",
       "\n",
       "[1849 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Cargar los datos\n",
    "flybondi_data = '../../data/cleaning_pipeline/after_translated.csv'\n",
    "df_cleaned = pd.read_csv(flybondi_data)\n",
    "\n",
    "# Filtrar las filas donde 'review_translated' es nulo o está vacío\n",
    "df_cleaned = df_cleaned[~(df_cleaned['review_translated'].isna() | (df_cleaned['review_translated'].astype(str).str.strip() == ''))]\n",
    "\n",
    "# Cambiar a stopwords en inglés\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Definir la puntuación, pero en vez de eliminarla, la reemplazamos con un espacio\n",
    "punctuation = string.punctuation + '¡'\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Expresión regular para detectar emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticones\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # símbolos de transporte y mapas\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # banderas\n",
    "        \"\\U00002702-\\U000027B0\"  # otros símbolos\n",
    "        \"\\U000024C2-\\U0001F251\"  # caracteres cerrados\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Asegurar que el texto no sea nulo o NaN\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Reemplazar puntuación con espacios en lugar de simplemente eliminarla\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \" \", text)\n",
    "\n",
    "    # Eliminar stopwords en inglés\n",
    "    text_words = text.split()\n",
    "    text = ' '.join([word for word in text_words if word not in english_stopwords])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Convertir todos los valores en 'review_translated' a string, y manejar NaN\n",
    "df_cleaned['review_translated'] = df_cleaned['review_translated'].astype(str)\n",
    "\n",
    "# Aplicar el preprocesamiento a la columna 'review_translated'\n",
    "df_cleaned['review_processed'] = df_cleaned['review_translated'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar las columnas procesadas\n",
    "df_cleaned[['review_translated', 'review_processed']]\n",
    "\n",
    "# Eliminar la columna 'review_translated'\n",
    "df_cleaned = df_cleaned.rename(columns={'language': 'source_language'})\n",
    "df_cleaned = df_cleaned.drop(columns=['review_translated', 'relevance_score', 'review', 'given_reviews', 'pictures', 'local_guide', 'likes'])\n",
    "\n",
    "# Renombrar 'review_processed' a 'review'\n",
    "df_cleaned = df_cleaned.rename(columns={'review_processed': 'review'})\n",
    "df_cleaned = df_cleaned.rename(columns={'relevance_score_normalized': 'relevance_score'})\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizador\n",
    "Ahora que tenemos un dataset con todas las reviews limpias y traducidas procedemos a lemmatizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgirod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 45.4MB/s]                    \n",
      "2024-10-20 20:51:41 INFO: Downloaded file to /home/jgirod/stanza_resources/resources.json\n",
      "2024-10-20 20:51:41 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-20 20:51:42 INFO: File exists: /home/jgirod/stanza_resources/en/default.zip\n",
      "2024-10-20 20:51:45 INFO: Finished downloading models and saved to /home/jgirod/stanza_resources\n",
      "2024-10-20 20:51:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 30.7MB/s]                    \n",
      "2024-10-20 20:51:45 INFO: Downloaded file to /home/jgirod/stanza_resources/resources.json\n",
      "2024-10-20 20:51:46 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-20 20:51:46 INFO: Using device: cpu\n",
      "2024-10-20 20:51:46 INFO: Loading: tokenize\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:46 INFO: Loading: mwt\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:46 INFO: Loading: pos\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: lemma\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: constituency\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: depparse\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: sentiment\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:47 INFO: Loading: ner\n",
      "/home/jgirod/.local/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-20 20:51:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "# Descargar el modelo de inglés\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def lemmatize_english(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "\n",
    "# Aplicar la función de lematización a las reviews en inglés\n",
    "df['review'] = df['review'].apply(lemmatize_english)\n",
    "\n",
    "# Guardar el DataFrame actualizado con las reviews lematizadas\n",
    "output_file_lemmatized_reviews = '../../data/en_cleaned_with_lemmatized_reviews.csv'\n",
    "df.to_csv(output_file_lemmatized_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
