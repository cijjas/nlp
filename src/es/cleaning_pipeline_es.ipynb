{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo de reviews scrapeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/final_combined_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a limpiar.\n",
    "\n",
    "1. Remover columnas in√∫tiles y filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnamed columns which are trash\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos ratings para que tenga unico formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>review_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>Silvia Elena P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Excelente el servicio. Tripulaci√≥n s√∫per atent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altamente recomendable. Equipo amable y eficie...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>Martina B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Todo un desastre, el vuelo de ida se atraso 3 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peor experiencia</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>carolina c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Que les puedo decir que no sepamos: es una aer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HDP AIRLINES</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>Jime Ache ü§©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quiero destacar que todo fue impecable en el v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Todo fue un 10</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Cami D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Desastre. Nos cambiaron el horario de vuelo mu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Desastre</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>Si quieren saber el significado de las palabra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SERVICIO NEFASTO FLYBONDI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>Aur√©lien C</td>\n",
       "      <td>6 opiniones</td>\n",
       "      <td>Compr√© un primer boleto, el cheque se cerr√≥ co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estafa total</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>La peor experience. Me cambiarion el horario d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>La peor basura voladora del mundo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>La empresa cumpli√≥ con las condiciones pautada...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Excelente la puntualidad, la atenci√≥n y el sev...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>2 opiniones</td>\n",
       "      <td>Realmente lo pensar√≠a dos veces antes de volve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Realmente lo pensar√≠a dos veces antes‚Ä¶</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1061 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name   experience  \\\n",
       "1300                Silvia Elena P          NaN   \n",
       "1301                     Martina B          NaN   \n",
       "1302                    carolina c          NaN   \n",
       "1303                   Jime Ache ü§©          NaN   \n",
       "1304                        Cami D          NaN   \n",
       "...                            ...          ...   \n",
       "2357                  Pablo Romero    1 opini√≥n   \n",
       "2358                    Aur√©lien C  6 opiniones   \n",
       "2359              Marcos Medvescig    1 opini√≥n   \n",
       "2360  Silvia Elena Perez Sbarbatti    1 opini√≥n   \n",
       "2361        Nazarena Sebastianelli  2 opiniones   \n",
       "\n",
       "                                            review_text  likes  \\\n",
       "1300  Excelente el servicio. Tripulaci√≥n s√∫per atent...    NaN   \n",
       "1301  Todo un desastre, el vuelo de ida se atraso 3 ...    NaN   \n",
       "1302  Que les puedo decir que no sepamos: es una aer...    NaN   \n",
       "1303  Quiero destacar que todo fue impecable en el v...    NaN   \n",
       "1304  Desastre. Nos cambiaron el horario de vuelo mu...    NaN   \n",
       "...                                                 ...    ...   \n",
       "2357  Si quieren saber el significado de las palabra...    NaN   \n",
       "2358  Compr√© un primer boleto, el cheque se cerr√≥ co...    NaN   \n",
       "2359  La peor experience. Me cambiarion el horario d...    NaN   \n",
       "2360  La empresa cumpli√≥ con las condiciones pautada...    NaN   \n",
       "2361  Realmente lo pensar√≠a dos veces antes de volve...    NaN   \n",
       "\n",
       "                                           review_title  rating  \n",
       "1300  Altamente recomendable. Equipo amable y eficie...     5.0  \n",
       "1301                                   Peor experiencia     1.0  \n",
       "1302                                       HDP AIRLINES     1.0  \n",
       "1303                                     Todo fue un 10     5.0  \n",
       "1304                                           Desastre     1.0  \n",
       "...                                                 ...     ...  \n",
       "2357                          SERVICIO NEFASTO FLYBONDI     1.0  \n",
       "2358                                       Estafa total     1.0  \n",
       "2359                  La peor basura voladora del mundo     1.0  \n",
       "2360  Excelente la puntualidad, la atenci√≥n y el sev...     5.0  \n",
       "2361             Realmente lo pensar√≠a dos veces antes‚Ä¶     3.0  \n",
       "\n",
       "[1061 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['rating_cleaned'] = df_cleaned['rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "df_cleaned = df_cleaned.drop(columns=['rating'])\n",
    "df_cleaned = df_cleaned.rename(columns={'rating_cleaned': 'rating'})\n",
    "df_cleaned['rating'] = df_cleaned['rating'].fillna(1.0)\n",
    "\n",
    "df_cleaned.iloc[1300:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos titulos con reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>servicio nefasto flybondi. si quieren saber el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>Aur√©lien C</td>\n",
       "      <td>6 opiniones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>estafa total. compr√© un primer boleto, el cheq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>la peor basura voladora del mundo. la peor exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>1 opini√≥n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excelente la puntualidad, la atenci√≥n y el sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>2 opiniones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realmente lo pensar√≠a dos veces antes‚Ä¶. realme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name   experience  likes  rating  \\\n",
       "2357                  Pablo Romero    1 opini√≥n    NaN     1.0   \n",
       "2358                    Aur√©lien C  6 opiniones    NaN     1.0   \n",
       "2359              Marcos Medvescig    1 opini√≥n    NaN     1.0   \n",
       "2360  Silvia Elena Perez Sbarbatti    1 opini√≥n    NaN     5.0   \n",
       "2361        Nazarena Sebastianelli  2 opiniones    NaN     3.0   \n",
       "\n",
       "                                                 review  \n",
       "2357  servicio nefasto flybondi. si quieren saber el...  \n",
       "2358  estafa total. compr√© un primer boleto, el cheq...  \n",
       "2359  la peor basura voladora del mundo. la peor exp...  \n",
       "2360  excelente la puntualidad, la atenci√≥n y el sev...  \n",
       "2361  realmente lo pensar√≠a dos veces antes‚Ä¶. realme...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['review'] = df_cleaned['review_title'].fillna('') + '. ' + df_cleaned['review_text'].fillna('')\n",
    "df_cleaned = df_cleaned.drop(columns=['review_title', 'review_text'])\n",
    "df_cleaned['review'] = df_cleaned['review'].str.lower()\n",
    "df_cleaned.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos reviews que tengan nombre repetido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_767151/2606435315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_cleaned = df_cleaned.groupby('name', group_keys=False).apply(longest_review)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1902, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def longest_review(group):\n",
    "    return group.loc[group['review'].str.len().idxmax()]\n",
    "\n",
    "df_cleaned = df_cleaned.groupby('name', group_keys=False).apply(longest_review)\n",
    "\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos \"Likes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>malisima. malisimo..la peor.\\nvuando fuimos pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>one of the worst we have flown. we have flown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>terrible service. flight was first delayed 25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rubbish low cost airline. bought 6 tickets via...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885David_R885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not refunding cancelled flights. not refunding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>–í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞</td>\n",
       "      <td>1 rese√±a</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. el vuelo fue reprogramado sin informarnos al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>–í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å</td>\n",
       "      <td>3 rese√±as</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. me gustar√≠a decirles a todos: nunca compren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>–î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞</td>\n",
       "      <td>1 rese√±a</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. esta empresa ya hace un a√±o que no viene, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>◊ô◊°◊û◊ô◊ü ◊ô</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dont buy here!!!! scam. it's completely ridicu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>ÏµúÏäπÌòÑ</td>\n",
       "      <td>4 rese√±as ¬∑ 2 fotos</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. nunca, nunca, nunca utilices esta aerol√≠nea....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name           experience  likes  rating  \\\n",
       "0           22fortinero                  NaN      0     1.0   \n",
       "1            23russellv                  NaN      0     2.0   \n",
       "2               4family                  NaN      0     1.0   \n",
       "3     5travellers602013                  NaN      0     1.0   \n",
       "4         885David_R885                  NaN      0     1.0   \n",
       "...                 ...                  ...    ...     ...   \n",
       "1897     –í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞             1 rese√±a      3     1.0   \n",
       "1898       –í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å            3 rese√±as      2     1.0   \n",
       "1899  –î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞             1 rese√±a      5     1.0   \n",
       "1900            ◊ô◊°◊û◊ô◊ü ◊ô                  NaN      0     1.0   \n",
       "1901                ÏµúÏäπÌòÑ  4 rese√±as ¬∑ 2 fotos      1     1.0   \n",
       "\n",
       "                                                 review  \n",
       "0     malisima. malisimo..la peor.\\nvuando fuimos pe...  \n",
       "1     one of the worst we have flown. we have flown ...  \n",
       "2     terrible service. flight was first delayed 25 ...  \n",
       "3     rubbish low cost airline. bought 6 tickets via...  \n",
       "4     not refunding cancelled flights. not refunding...  \n",
       "...                                                 ...  \n",
       "1897  . el vuelo fue reprogramado sin informarnos al...  \n",
       "1898  . me gustar√≠a decirles a todos: nunca compren ...  \n",
       "1899  . esta empresa ya hace un a√±o que no viene, fu...  \n",
       "1900  dont buy here!!!! scam. it's completely ridicu...  \n",
       "1901  . nunca, nunca, nunca utilices esta aerol√≠nea....  \n",
       "\n",
       "[1902 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['likes'] = df_cleaned['likes'].fillna(0)\n",
    "df_cleaned['likes'] = df_cleaned['likes'].astype(int)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n que transforma la experience y los likes en un √∫nico formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_experience(experience):\n",
    "    resenas = 0\n",
    "    fotos = 0\n",
    "    local_guide = 0\n",
    "\n",
    "    if pd.isna(experience):\n",
    "        return resenas, fotos, local_guide\n",
    "\n",
    "    if 'Local Guide' in experience:\n",
    "        local_guide = 1\n",
    "\n",
    "    resenas_match = re.search(r'(\\d+[\\.,]?\\d*) (rese√±as|opini√≥n|opiniones)', experience)\n",
    "    if resenas_match:\n",
    "        resenas = int(resenas_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    fotos_match = re.search(r'(\\d+[\\.,]?\\d*) fotos', experience)\n",
    "    if fotos_match:\n",
    "        fotos = int(fotos_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    return resenas, fotos, local_guide\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']] = df_cleaned['experience'].apply(\n",
    "    lambda x: pd.Series(parse_experience(x))\n",
    ")\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']]\n",
    "df_cleaned = df_cleaned.drop(columns=['experience'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>given_reviews</th>\n",
       "      <th>pictures</th>\n",
       "      <th>local_guide</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_score_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.001745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.002909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.004479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      likes  given_reviews  pictures  local_guide  relevance_score  \\\n",
       "0         0              0         0            0             0.00   \n",
       "1         0              0         0            0             0.00   \n",
       "2         0              0         0            0             0.00   \n",
       "3         0              0         0            0             0.00   \n",
       "4         0              0         0            0             0.00   \n",
       "...     ...            ...       ...          ...              ...   \n",
       "1897      3              0         0            0             0.90   \n",
       "1898      2              3         0            0             2.10   \n",
       "1899      5              0         0            0             1.50   \n",
       "1900      0              0         0            0             0.00   \n",
       "1901      1              4         2            0             2.31   \n",
       "\n",
       "      relevance_score_normalized  \n",
       "0                       0.000000  \n",
       "1                       0.000000  \n",
       "2                       0.000000  \n",
       "3                       0.000000  \n",
       "4                       0.000000  \n",
       "...                          ...  \n",
       "1897                    0.001745  \n",
       "1898                    0.004072  \n",
       "1899                    0.002909  \n",
       "1900                    0.000000  \n",
       "1901                    0.004479  \n",
       "\n",
       "[1902 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_relevance(row, W_l=0.3, W_r=0.5, W_p=0.005, W_lg=0.5):\n",
    "    relevance = (\n",
    "        W_l * row['likes'] +\n",
    "        W_r * row['given_reviews'] +\n",
    "        W_p * row['pictures'] +\n",
    "        W_lg * row['local_guide']\n",
    "    )\n",
    "    return relevance\n",
    "\n",
    "df_cleaned['relevance_score'] = df_cleaned.apply(calculate_relevance, axis=1)\n",
    "df_cleaned['relevance_score_normalized'] = (df_cleaned['relevance_score'] - df_cleaned['relevance_score'].min()) / (df_cleaned['relevance_score'].max() - df_cleaned['relevance_score'].min())\n",
    "\n",
    "df_cleaned[['likes', 'given_reviews', 'pictures', 'local_guide', 'relevance_score', 'relevance_score_normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>given_reviews</th>\n",
       "      <th>pictures</th>\n",
       "      <th>local_guide</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_score_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>malisima. malisimo..la peor.\\nvuando fuimos pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>one of the worst we have flown. we have flown ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>terrible service. flight was first delayed 25 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rubbish low cost airline. bought 6 tickets via...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885David_R885</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not refunding cancelled flights. not refunding...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>–í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. el vuelo fue reprogramado sin informarnos al...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.001745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>–í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. me gustar√≠a decirles a todos: nunca compren ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>–î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. esta empresa ya hace un a√±o que no viene, fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.002909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>◊ô◊°◊û◊ô◊ü ◊ô</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dont buy here!!!! scam. it's completely ridicu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>ÏµúÏäπÌòÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. nunca, nunca, nunca utilices esta aerol√≠nea....</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.004479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  likes  rating  \\\n",
       "0           22fortinero      0     1.0   \n",
       "1            23russellv      0     2.0   \n",
       "2               4family      0     1.0   \n",
       "3     5travellers602013      0     1.0   \n",
       "4         885David_R885      0     1.0   \n",
       "...                 ...    ...     ...   \n",
       "1897     –í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞      3     1.0   \n",
       "1898       –í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å      2     1.0   \n",
       "1899  –î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞      5     1.0   \n",
       "1900            ◊ô◊°◊û◊ô◊ü ◊ô      0     1.0   \n",
       "1901                ÏµúÏäπÌòÑ      1     1.0   \n",
       "\n",
       "                                                 review  given_reviews  \\\n",
       "0     malisima. malisimo..la peor.\\nvuando fuimos pe...              0   \n",
       "1     one of the worst we have flown. we have flown ...              0   \n",
       "2     terrible service. flight was first delayed 25 ...              0   \n",
       "3     rubbish low cost airline. bought 6 tickets via...              0   \n",
       "4     not refunding cancelled flights. not refunding...              0   \n",
       "...                                                 ...            ...   \n",
       "1897  . el vuelo fue reprogramado sin informarnos al...              0   \n",
       "1898  . me gustar√≠a decirles a todos: nunca compren ...              3   \n",
       "1899  . esta empresa ya hace un a√±o que no viene, fu...              0   \n",
       "1900  dont buy here!!!! scam. it's completely ridicu...              0   \n",
       "1901  . nunca, nunca, nunca utilices esta aerol√≠nea....              4   \n",
       "\n",
       "      pictures  local_guide  relevance_score  relevance_score_normalized  \n",
       "0            0            0             0.00                    0.000000  \n",
       "1            0            0             0.00                    0.000000  \n",
       "2            0            0             0.00                    0.000000  \n",
       "3            0            0             0.00                    0.000000  \n",
       "4            0            0             0.00                    0.000000  \n",
       "...        ...          ...              ...                         ...  \n",
       "1897         0            0             0.90                    0.001745  \n",
       "1898         0            0             2.10                    0.004072  \n",
       "1899         0            0             1.50                    0.002909  \n",
       "1900         0            0             0.00                    0.000000  \n",
       "1901         2            0             2.31                    0.004479  \n",
       "\n",
       "[1902 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m LangDetectException:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Handle cases where language detection fails\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the new 'language' column\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df_cleaned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mdetect_language\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_language\u001b[39m(text):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns a language code (e.g., 'en', 'es')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m LangDetectException:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/langdetect/detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m    129\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/langdetect/detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    which has the highest probability.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probabilities:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlang\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/langdetect/detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/langdetect/detector.py:163\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_lang_prob(prob, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(ngrams), alpha)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONV_THRESHOLD \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mITERATION_LIMIT:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/langdetect/detector.py:223\u001b[0m, in \u001b[0;36mDetector._normalize_prob\u001b[0;34m(self, prob)\u001b[0m\n\u001b[1;32m    220\u001b[0m             result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanglist[j], p)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_normalize_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, prob):\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Normalize probabilities and check convergence by the maximun probability.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     maxp, sump \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28msum\u001b[39m(prob)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)  # Returns a language code (e.g., 'en', 'es')\n",
    "    except LangDetectException:\n",
    "        return 'unknown'  # Handle cases where language detection fails\n",
    "\n",
    "df_cleaned['language'] = df_cleaned['review'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame with the new 'language' column\n",
    "df_cleaned[['review', 'language']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_spanish(text, src_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='es')  # 'es' for Spanish\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(\"could not translate: \", text)\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text\n",
    "\n",
    "def translate_non_spanish(text, lang):\n",
    "    if lang != 'es' and lang != 'unknown':\n",
    "        return translate_to_spanish(text, lang)\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned.apply(\n",
    "    lambda row: translate_non_spanish(row['review'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_cleaned[['review', 'language', 'review_translated']]\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "output_file_translated_reviews = '../data/cleaned_with_translated_non_es_reviews.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22fortinero</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>malisima malisimola peor\\nvuando perdi dia hot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23russellv</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>uno peores voladohemos volado cientos aerol√≠ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4family</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>terrible servicioel vuelo retras√≥ primera vez ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5travellers602013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>basco aerol√≠nea bajo costocompr√© 6 boletos tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>885David_R885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no reembolsar vuelos canceladosno reembolsar v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>–í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>vuelo reprogramado informarnos respecto pagam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>–í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>gustar√≠a decirles nunca compren mosca flybond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>–î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>empresa hace a√±o viene p√©rdida tiempo venir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>◊ô◊°◊û◊ô◊ü ◊ô</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>estafaes completamente rid√≠culo vuelo retras√≥ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>ÏµúÏäπÌòÑ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>nunca nunca nunca utilices aerol√≠nea\\ncuando ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1902 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  rating  relevance_score  \\\n",
       "0           22fortinero     1.0         0.000000   \n",
       "1            23russellv     2.0         0.000000   \n",
       "2               4family     1.0         0.000000   \n",
       "3     5travellers602013     1.0         0.000000   \n",
       "4         885David_R885     1.0         0.000000   \n",
       "...                 ...     ...              ...   \n",
       "1897     –í–∞–ª–µ—Ä–∏—è –®—É–ª—å–≥–∞     1.0         0.001745   \n",
       "1898       –í–∏–∫–∞ –ú–µ–≥–∞–ª–∏—Å     1.0         0.004072   \n",
       "1899  –î–∞—Ä—å—è –í–µ–Ω–µ–¥–∏–∫—Ç–æ–≤–∞     1.0         0.002909   \n",
       "1900            ◊ô◊°◊û◊ô◊ü ◊ô     1.0         0.000000   \n",
       "1901                ÏµúÏäπÌòÑ     1.0         0.004479   \n",
       "\n",
       "                                                 review  \n",
       "0     malisima malisimola peor\\nvuando perdi dia hot...  \n",
       "1     uno peores voladohemos volado cientos aerol√≠ne...  \n",
       "2     terrible servicioel vuelo retras√≥ primera vez ...  \n",
       "3     basco aerol√≠nea bajo costocompr√© 6 boletos tra...  \n",
       "4     no reembolsar vuelos canceladosno reembolsar v...  \n",
       "...                                                 ...  \n",
       "1897   vuelo reprogramado informarnos respecto pagam...  \n",
       "1898   gustar√≠a decirles nunca compren mosca flybond...  \n",
       "1899        empresa hace a√±o viene p√©rdida tiempo venir  \n",
       "1900  estafaes completamente rid√≠culo vuelo retras√≥ ...  \n",
       "1901   nunca nunca nunca utilices aerol√≠nea\\ncuando ...  \n",
       "\n",
       "[1902 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "flybondi_data = '../data/cleaned_with_translated_non_es_reviews.csv'\n",
    "df_cleaned = pd.read_csv(flybondi_data)\n",
    "\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "punctuation = string.punctuation + '¬°'\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002702-\\U000027B0\"  # other symbols\n",
    "        \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    # Remove punctuation by translating all punctuation characters to None\n",
    "    text = text.translate(str.maketrans('', '', punctuation))\n",
    "    # Remove Spanish stopwords by iterating through the words\n",
    "    for stopword in spanish_stopwords:\n",
    "        text = text.replace(f\" {stopword} \", \" \")  # Replace only whole words\n",
    "    return text\n",
    "\n",
    "df_cleaned['review_processed'] = df_cleaned['review_translated'].apply(preprocess_text)\n",
    "\n",
    "df_cleaned[['review_translated', 'review_processed']]\n",
    "# remove review_translated column\n",
    "df_cleaned = df_cleaned.drop(columns=['review_translated', 'language', 'relevance_score', 'review', 'given_reviews', 'pictures', 'local_guide', 'likes'])\n",
    "#rename review_processed to review\n",
    "df_cleaned = df_cleaned.rename(columns={'review_processed': 'review'})\n",
    "df_cleaned = df_cleaned.rename(columns={'relevance_score_normalized': 'relevance_score'})\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizador\n",
    "Ahora que tenemos un dataset con todas las reviews limpias y traducidas procedemos a lemmatizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaquin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 51.9MB/s]                    \n",
      "2024-10-18 21:08:58 INFO: Downloaded file to /home/joaquin/stanza_resources/resources.json\n",
      "2024-10-18 21:08:58 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2024-10-18 21:09:00 INFO: File exists: /home/joaquin/stanza_resources/es/default.zip\n",
      "2024-10-18 21:09:06 INFO: Finished downloading models and saved to /home/joaquin/stanza_resources\n",
      "2024-10-18 21:09:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 42.5MB/s]                    \n",
      "2024-10-18 21:09:06 INFO: Downloaded file to /home/joaquin/stanza_resources/resources.json\n",
      "2024-10-18 21:09:07 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2024-10-18 21:09:07 INFO: Using device: cuda\n",
      "2024-10-18 21:09:07 INFO: Loading: tokenize\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:08 INFO: Loading: mwt\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:08 INFO: Loading: pos\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:08 INFO: Loading: lemma\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:08 INFO: Loading: constituency\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:08 INFO: Loading: depparse\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:09 INFO: Loading: sentiment\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:09 INFO: Loading: ner\n",
      "/home/joaquin/.local/lib/python3.10/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 21:09:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "stanza.download('es')\n",
    "nlp = stanza.Pipeline('es')\n",
    "\n",
    "def lemmatize_spanish(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "# Apply the lemmatization function only to Spanish reviews\n",
    "df['review'] = df.apply(\n",
    "    lambda row: lemmatize_spanish(row['review']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optionally, save the updated DataFrame with lemmatized reviews\n",
    "output_file_lemmatized_reviews = '../data/cleaned_with_lemmatized_reviews.csv'\n",
    "df.to_csv(output_file_lemmatized_reviews, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
