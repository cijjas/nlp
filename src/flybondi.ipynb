{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo de reviews scrapeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/all_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos a limpiar.\n",
    "\n",
    "1. Remover columnas inútiles y filas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnamed columns which are trash\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# remove duplicates\n",
    "df_cleaned = df_cleaned.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos ratings para que tenga unico formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>review_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>RUBEN OMAR N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Es una aerolínea de bajo costo, los servicios ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No esperaba nada mas por el precio del Ticket</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>Julio F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Realmente sorprendido por ser una aerolineas L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muy Bueno</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>Nelson G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muchas representaciones de vuelos, tanto de id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Desastre</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>victoriaguida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Súper económico y el servicio no tiene nada qu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buen servicio</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>hector_regina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nadir te avisa nada, teniamos que viajar y el ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUY MAL TRATO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>Si quieren saber el significado de las palabra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SERVICIO NEFASTO FLYBONDI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Aurélien C</td>\n",
       "      <td>6 opiniones</td>\n",
       "      <td>Compré un primer boleto, el cheque se cerró co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estafa total</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>La peor experience. Me cambiarion el horario d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>La peor basura voladora del mundo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>La empresa cumplió con las condiciones pautada...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Excelente la puntualidad, la atención y el sev...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>2 opiniones</td>\n",
       "      <td>Realmente lo pensaría dos veces antes de volve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Realmente lo pensaría dos veces antes…</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>563 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name   experience  \\\n",
       "1300                  RUBEN OMAR N          NaN   \n",
       "1301                       Julio F          NaN   \n",
       "1302                      Nelson G          NaN   \n",
       "1303                 victoriaguida          NaN   \n",
       "1304                 hector_regina          NaN   \n",
       "...                            ...          ...   \n",
       "1859                  Pablo Romero    1 opinión   \n",
       "1860                    Aurélien C  6 opiniones   \n",
       "1861              Marcos Medvescig    1 opinión   \n",
       "1862  Silvia Elena Perez Sbarbatti    1 opinión   \n",
       "1863        Nazarena Sebastianelli  2 opiniones   \n",
       "\n",
       "                                            review_text  likes  \\\n",
       "1300  Es una aerolínea de bajo costo, los servicios ...    NaN   \n",
       "1301  Realmente sorprendido por ser una aerolineas L...    NaN   \n",
       "1302  Muchas representaciones de vuelos, tanto de id...    NaN   \n",
       "1303  Súper económico y el servicio no tiene nada qu...    NaN   \n",
       "1304  Nadir te avisa nada, teniamos que viajar y el ...    NaN   \n",
       "...                                                 ...    ...   \n",
       "1859  Si quieren saber el significado de las palabra...    NaN   \n",
       "1860  Compré un primer boleto, el cheque se cerró co...    NaN   \n",
       "1861  La peor experience. Me cambiarion el horario d...    NaN   \n",
       "1862  La empresa cumplió con las condiciones pautada...    NaN   \n",
       "1863  Realmente lo pensaría dos veces antes de volve...    NaN   \n",
       "\n",
       "                                           review_title  rating  \n",
       "1300      No esperaba nada mas por el precio del Ticket     3.0  \n",
       "1301                                          Muy Bueno     4.0  \n",
       "1302                                           Desastre     1.0  \n",
       "1303                                      Buen servicio     4.0  \n",
       "1304                                      MUY MAL TRATO     1.0  \n",
       "...                                                 ...     ...  \n",
       "1859                          SERVICIO NEFASTO FLYBONDI     1.0  \n",
       "1860                                       Estafa total     1.0  \n",
       "1861                  La peor basura voladora del mundo     1.0  \n",
       "1862  Excelente la puntualidad, la atención y el sev...     5.0  \n",
       "1863             Realmente lo pensaría dos veces antes…     3.0  \n",
       "\n",
       "[563 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['rating_cleaned'] = df_cleaned['rating'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "df_cleaned = df_cleaned.drop(columns=['rating'])\n",
    "df_cleaned = df_cleaned.rename(columns={'rating_cleaned': 'rating'})\n",
    "df_cleaned['rating'] = df_cleaned['rating'].fillna(1.0)\n",
    "\n",
    "df_cleaned.iloc[1300:] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos titulos con reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>servicio nefasto flybondi. si quieren saber el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Aurélien C</td>\n",
       "      <td>6 opiniones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>estafa total. compré un primer boleto, el cheq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>la peor basura voladora del mundo. la peor exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excelente la puntualidad, la atención y el sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>2 opiniones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realmente lo pensaría dos veces antes…. realme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name   experience  likes  rating  \\\n",
       "1859                  Pablo Romero    1 opinión    NaN     1.0   \n",
       "1860                    Aurélien C  6 opiniones    NaN     1.0   \n",
       "1861              Marcos Medvescig    1 opinión    NaN     1.0   \n",
       "1862  Silvia Elena Perez Sbarbatti    1 opinión    NaN     5.0   \n",
       "1863        Nazarena Sebastianelli  2 opiniones    NaN     3.0   \n",
       "\n",
       "                                                 review  \n",
       "1859  servicio nefasto flybondi. si quieren saber el...  \n",
       "1860  estafa total. compré un primer boleto, el cheq...  \n",
       "1861  la peor basura voladora del mundo. la peor exp...  \n",
       "1862  excelente la puntualidad, la atención y el sev...  \n",
       "1863  realmente lo pensaría dos veces antes…. realme...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['review'] = df_cleaned['review_title'].fillna('') + '. ' + df_cleaned['review_text'].fillna('')\n",
    "df_cleaned = df_cleaned.drop(columns=['review_title', 'review_text'])\n",
    "df_cleaned['review'] = df_cleaned['review'].str.lower()\n",
    "df_cleaned.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos \"Likes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>experience</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vanesa Fraccarolli</td>\n",
       "      <td>4 reseñas · 7 fotos</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. impecable la experiencia con flybondi ida y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Betiana Tetti</td>\n",
       "      <td>Local Guide · 703 reseñas · 3.139 fotos</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. la verdad que siempre que viajé flybondi fue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valeria Simplituca</td>\n",
       "      <td>22 reseñas · 13 fotos</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. una vergüenza y un desastre!!! cambian las c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aldana Santandreu</td>\n",
       "      <td>6 reseñas · 6 fotos</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. tanto el vuelo de ida como de vuelta a baril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elian Pinel</td>\n",
       "      <td>Local Guide · 104 reseñas · 422 fotos</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. viajamos sin ningún tipo de contratiempo a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>servicio nefasto flybondi. si quieren saber el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Aurélien C</td>\n",
       "      <td>6 opiniones</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>estafa total. compré un primer boleto, el cheq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>la peor basura voladora del mundo. la peor exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>1 opinión</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excelente la puntualidad, la atención y el sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>2 opiniones</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realmente lo pensaría dos veces antes…. realme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name                               experience  \\\n",
       "0               Vanesa Fraccarolli                      4 reseñas · 7 fotos   \n",
       "1                    Betiana Tetti  Local Guide · 703 reseñas · 3.139 fotos   \n",
       "2               Valeria Simplituca                    22 reseñas · 13 fotos   \n",
       "3                Aldana Santandreu                      6 reseñas · 6 fotos   \n",
       "4                      Elian Pinel    Local Guide · 104 reseñas · 422 fotos   \n",
       "...                            ...                                      ...   \n",
       "1859                  Pablo Romero                                1 opinión   \n",
       "1860                    Aurélien C                              6 opiniones   \n",
       "1861              Marcos Medvescig                                1 opinión   \n",
       "1862  Silvia Elena Perez Sbarbatti                                1 opinión   \n",
       "1863        Nazarena Sebastianelli                              2 opiniones   \n",
       "\n",
       "      likes  rating                                             review  \n",
       "0         0     5.0  . impecable la experiencia con flybondi ida y ...  \n",
       "1         3     5.0  . la verdad que siempre que viajé flybondi fue...  \n",
       "2         4     1.0  . una vergüenza y un desastre!!! cambian las c...  \n",
       "3         1     5.0  . tanto el vuelo de ida como de vuelta a baril...  \n",
       "4         3     5.0  . viajamos sin ningún tipo de contratiempo a b...  \n",
       "...     ...     ...                                                ...  \n",
       "1859      0     1.0  servicio nefasto flybondi. si quieren saber el...  \n",
       "1860      0     1.0  estafa total. compré un primer boleto, el cheq...  \n",
       "1861      0     1.0  la peor basura voladora del mundo. la peor exp...  \n",
       "1862      0     5.0  excelente la puntualidad, la atención y el sev...  \n",
       "1863      0     3.0  realmente lo pensaría dos veces antes…. realme...  \n",
       "\n",
       "[1863 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned['likes'] = df_cleaned['likes'].fillna(0)\n",
    "df_cleaned['likes'] = df_cleaned['likes'].astype(int)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que transforma la experience y los likes en un único formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      given_reviews  pictures  local_guide\n",
      "0                 4         7            0\n",
      "1               703      3139            1\n",
      "2                22        13            0\n",
      "3                 6         6            0\n",
      "4               104       422            1\n",
      "...             ...       ...          ...\n",
      "1859              1         0            0\n",
      "1860              6         0            0\n",
      "1861              1         0            0\n",
      "1862              1         0            0\n",
      "1863              2         0            0\n",
      "\n",
      "[1863 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_experience(experience):\n",
    "    resenas = 0\n",
    "    fotos = 0\n",
    "    local_guide = 0\n",
    "\n",
    "    if pd.isna(experience):\n",
    "        return resenas, fotos, local_guide\n",
    "\n",
    "    if 'Local Guide' in experience:\n",
    "        local_guide = 1\n",
    "\n",
    "    resenas_match = re.search(r'(\\d+[\\.,]?\\d*) (reseñas|opinión|opiniones)', experience)\n",
    "    if resenas_match:\n",
    "        resenas = int(resenas_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    fotos_match = re.search(r'(\\d+[\\.,]?\\d*) fotos', experience)\n",
    "    if fotos_match:\n",
    "        fotos = int(fotos_match.group(1).replace('.', '').replace(',', '.'))\n",
    "\n",
    "    return resenas, fotos, local_guide\n",
    "\n",
    "df_cleaned[['given_reviews', 'pictures', 'local_guide']] = df_cleaned['experience'].apply(\n",
    "    lambda x: pd.Series(parse_experience(x))\n",
    ")\n",
    "\n",
    "print(df_cleaned[['given_reviews', 'pictures', 'local_guide']])\n",
    "df_cleaned = df_cleaned.drop(columns=['experience'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      likes  given_reviews  pictures  local_guide  relevance_score  \\\n",
      "0         0              4         7            0            2.035   \n",
      "1         3            703      3139            1          368.595   \n",
      "2         4             22        13            0           12.265   \n",
      "3         1              6         6            0            3.330   \n",
      "4         3            104       422            1           55.510   \n",
      "...     ...            ...       ...          ...              ...   \n",
      "1859      0              1         0            0            0.500   \n",
      "1860      0              6         0            0            3.000   \n",
      "1861      0              1         0            0            0.500   \n",
      "1862      0              1         0            0            0.500   \n",
      "1863      0              2         0            0            1.000   \n",
      "\n",
      "      relevance_score_normalized  \n",
      "0                       0.003946  \n",
      "1                       0.714761  \n",
      "2                       0.023784  \n",
      "3                       0.006457  \n",
      "4                       0.107642  \n",
      "...                          ...  \n",
      "1859                    0.000970  \n",
      "1860                    0.005817  \n",
      "1861                    0.000970  \n",
      "1862                    0.000970  \n",
      "1863                    0.001939  \n",
      "\n",
      "[1863 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_relevance(row, W_l=0.3, W_r=0.5, W_p=0.005, W_lg=0.5):\n",
    "    relevance = (\n",
    "        W_l * row['likes'] +       \n",
    "        W_r * row['given_reviews'] +     \n",
    "        W_p * row['pictures'] +       \n",
    "        W_lg * row['local_guide']  \n",
    "    )\n",
    "    return relevance\n",
    "\n",
    "df_cleaned['relevance_score'] = df_cleaned.apply(calculate_relevance, axis=1)\n",
    "df_cleaned['relevance_score_normalized'] = (df_cleaned['relevance_score'] - df_cleaned['relevance_score'].min()) / (df_cleaned['relevance_score'].max() - df_cleaned['relevance_score'].min())\n",
    "\n",
    "print(df_cleaned[['likes', 'given_reviews', 'pictures', 'local_guide', 'relevance_score', 'relevance_score_normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>likes</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>given_reviews</th>\n",
       "      <th>pictures</th>\n",
       "      <th>local_guide</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_score_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vanesa Fraccarolli</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. impecable la experiencia con flybondi ida y ...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.035</td>\n",
       "      <td>0.003946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Betiana Tetti</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. la verdad que siempre que viajé flybondi fue...</td>\n",
       "      <td>703</td>\n",
       "      <td>3139</td>\n",
       "      <td>1</td>\n",
       "      <td>368.595</td>\n",
       "      <td>0.714761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valeria Simplituca</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>. una vergüenza y un desastre!!! cambian las c...</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>12.265</td>\n",
       "      <td>0.023784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aldana Santandreu</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. tanto el vuelo de ida como de vuelta a baril...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3.330</td>\n",
       "      <td>0.006457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elian Pinel</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>. viajamos sin ningún tipo de contratiempo a b...</td>\n",
       "      <td>104</td>\n",
       "      <td>422</td>\n",
       "      <td>1</td>\n",
       "      <td>55.510</td>\n",
       "      <td>0.107642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>Pablo Romero</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>servicio nefasto flybondi. si quieren saber el...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Aurélien C</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>estafa total. compré un primer boleto, el cheq...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.005817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Marcos Medvescig</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>la peor basura voladora del mundo. la peor exp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Silvia Elena Perez Sbarbatti</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excelente la puntualidad, la atención y el sev...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>Nazarena Sebastianelli</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realmente lo pensaría dos veces antes…. realme...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  likes  rating  \\\n",
       "0               Vanesa Fraccarolli      0     5.0   \n",
       "1                    Betiana Tetti      3     5.0   \n",
       "2               Valeria Simplituca      4     1.0   \n",
       "3                Aldana Santandreu      1     5.0   \n",
       "4                      Elian Pinel      3     5.0   \n",
       "...                            ...    ...     ...   \n",
       "1859                  Pablo Romero      0     1.0   \n",
       "1860                    Aurélien C      0     1.0   \n",
       "1861              Marcos Medvescig      0     1.0   \n",
       "1862  Silvia Elena Perez Sbarbatti      0     5.0   \n",
       "1863        Nazarena Sebastianelli      0     3.0   \n",
       "\n",
       "                                                 review  given_reviews  \\\n",
       "0     . impecable la experiencia con flybondi ida y ...              4   \n",
       "1     . la verdad que siempre que viajé flybondi fue...            703   \n",
       "2     . una vergüenza y un desastre!!! cambian las c...             22   \n",
       "3     . tanto el vuelo de ida como de vuelta a baril...              6   \n",
       "4     . viajamos sin ningún tipo de contratiempo a b...            104   \n",
       "...                                                 ...            ...   \n",
       "1859  servicio nefasto flybondi. si quieren saber el...              1   \n",
       "1860  estafa total. compré un primer boleto, el cheq...              6   \n",
       "1861  la peor basura voladora del mundo. la peor exp...              1   \n",
       "1862  excelente la puntualidad, la atención y el sev...              1   \n",
       "1863  realmente lo pensaría dos veces antes…. realme...              2   \n",
       "\n",
       "      pictures  local_guide  relevance_score  relevance_score_normalized  \n",
       "0            7            0            2.035                    0.003946  \n",
       "1         3139            1          368.595                    0.714761  \n",
       "2           13            0           12.265                    0.023784  \n",
       "3            6            0            3.330                    0.006457  \n",
       "4          422            1           55.510                    0.107642  \n",
       "...        ...          ...              ...                         ...  \n",
       "1859         0            0            0.500                    0.000970  \n",
       "1860         0            0            3.000                    0.005817  \n",
       "1861         0            0            0.500                    0.000970  \n",
       "1862         0            0            0.500                    0.000970  \n",
       "1863         0            0            1.000                    0.001939  \n",
       "\n",
       "[1863 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review language\n",
      "0     . impecable la experiencia con flybondi ida y ...       es\n",
      "1     . la verdad que siempre que viajé flybondi fue...       es\n",
      "2     . una vergüenza y un desastre!!! cambian las c...       es\n",
      "3     . tanto el vuelo de ida como de vuelta a baril...       es\n",
      "4     . viajamos sin ningún tipo de contratiempo a b...       es\n",
      "...                                                 ...      ...\n",
      "1859  servicio nefasto flybondi. si quieren saber el...       es\n",
      "1860  estafa total. compré un primer boleto, el cheq...       es\n",
      "1861  la peor basura voladora del mundo. la peor exp...       es\n",
      "1862  excelente la puntualidad, la atención y el sev...       es\n",
      "1863  realmente lo pensaría dos veces antes…. realme...       es\n",
      "\n",
      "[1863 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)  # Returns a language code (e.g., 'en', 'es')\n",
    "    except LangDetectException:\n",
    "        return 'unknown'  # Handle cases where language detection fails\n",
    "\n",
    "df_cleaned['language'] = df_cleaned['review'].apply(detect_language)\n",
    "\n",
    "# Display the DataFrame with the new 'language' column\n",
    "print(df_cleaned[['review', 'language']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "Error translating: 'Translator' object has no attribute 'raise_Exception'\n",
      "                                                 review language  \\\n",
      "0     . impecable la experiencia con flybondi ida y ...       es   \n",
      "1     . la verdad que siempre que viajé flybondi fue...       es   \n",
      "2     . una vergüenza y un desastre!!! cambian las c...       es   \n",
      "3     . tanto el vuelo de ida como de vuelta a baril...       es   \n",
      "4     . viajamos sin ningún tipo de contratiempo a b...       es   \n",
      "...                                                 ...      ...   \n",
      "1859  servicio nefasto flybondi. si quieren saber el...       es   \n",
      "1860  estafa total. compré un primer boleto, el cheq...       es   \n",
      "1861  la peor basura voladora del mundo. la peor exp...       es   \n",
      "1862  excelente la puntualidad, la atención y el sev...       es   \n",
      "1863  realmente lo pensaría dos veces antes…. realme...       es   \n",
      "\n",
      "                                      review_translated  \n",
      "0     . impecable la experiencia con flybondi ida y ...  \n",
      "1     . la verdad que siempre que viajé flybondi fue...  \n",
      "2     . una vergüenza y un desastre!!! cambian las c...  \n",
      "3     . tanto el vuelo de ida como de vuelta a baril...  \n",
      "4     . viajamos sin ningún tipo de contratiempo a b...  \n",
      "...                                                 ...  \n",
      "1859  servicio nefasto flybondi. si quieren saber el...  \n",
      "1860  estafa total. compré un primer boleto, el cheq...  \n",
      "1861  la peor basura voladora del mundo. la peor exp...  \n",
      "1862  excelente la puntualidad, la atención y el sev...  \n",
      "1863  realmente lo pensaría dos veces antes…. realme...  \n",
      "\n",
      "[1863 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_spanish(text, src_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=src_lang, dest='es')  # 'es' for Spanish\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(\"could not translate: \", text)\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text\n",
    "    \n",
    "def translate_non_spanish(text, lang):\n",
    "    if lang != 'es' and lang != 'unknown':\n",
    "        return translate_to_spanish(text, lang)\n",
    "    return text \n",
    "\n",
    "df_cleaned['review_translated'] = df_cleaned.apply(\n",
    "    lambda row: translate_non_spanish(row['review'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_cleaned[['review', 'language', 'review_translated']])\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "output_file_translated_reviews = 'cleaned_with_translated_non_es_reviews.csv'\n",
    "df_cleaned.to_csv(output_file_translated_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizador\n",
    "Ahora que tenemos un dataset con todas las reviews limpias y traducidas procedemos a lemmatizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 24.0MB/s]                    \n",
      "2024-10-18 13:15:04 INFO: Downloaded file to /home/chris/stanza_resources/resources.json\n",
      "2024-10-18 13:15:04 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2024-10-18 13:15:05 INFO: File exists: /home/chris/stanza_resources/es/default.zip\n",
      "2024-10-18 13:15:21 INFO: Finished downloading models and saved to /home/chris/stanza_resources\n",
      "2024-10-18 13:15:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 14.1MB/s]                    \n",
      "2024-10-18 13:15:21 INFO: Downloaded file to /home/chris/stanza_resources/resources.json\n",
      "2024-10-18 13:15:23 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2024-10-18 13:15:23 INFO: Using device: cuda\n",
      "2024-10-18 13:15:23 INFO: Loading: tokenize\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:23 INFO: Loading: mwt\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:23 INFO: Loading: pos\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:24 INFO: Loading: lemma\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:24 INFO: Loading: constituency\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:25 INFO: Loading: depparse\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:25 INFO: Loading: sentiment\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:26 INFO: Loading: ner\n",
      "/home/chris/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-10-18 13:15:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;241m.\u001b[39mlemma \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mwords])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply the lemmatization function only to Spanish reviews\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize_spanish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_translated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the original, translated, and lemmatized reviews\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_translated\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;241m.\u001b[39mlemma \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mwords])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply the lemmatization function only to Spanish reviews\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mlemmatize_spanish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_translated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     18\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the original, translated, and lemmatized reviews\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_translated\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mlemmatize_spanish\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_spanish\u001b[39m(text):\n\u001b[0;32m---> 11\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;241m.\u001b[39mlemma \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mwords])\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/pipeline/depparse_processor.py:65\u001b[0m, in \u001b[0;36mDepparseProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     63\u001b[0m     preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m---> 65\u001b[0m         preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mdata_orig_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     preds \u001b[38;5;241m=\u001b[39m unsort(preds, batch\u001b[38;5;241m.\u001b[39mdata_orig_idx)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/depparse/trainer.py:149\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    148\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeprel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m head_seqs \u001b[38;5;241m=\u001b[39m [chuliu_edmonds_one_root(adj[:l, :l])[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m adj, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m], sentlens)] \u001b[38;5;66;03m# remove attachment for the root\u001b[39;00m\n\u001b[1;32m    151\u001b[0m deprel_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeprel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap([preds[\u001b[38;5;241m1\u001b[39m][i][j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][h] \u001b[38;5;28;01mfor\u001b[39;00m j, h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hs)]) \u001b[38;5;28;01mfor\u001b[39;00m i, hs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(head_seqs)]\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/depparse/model.py:206\u001b[0m, in \u001b[0;36mParser.forward\u001b[0;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens, text)\u001b[0m\n\u001b[1;32m    202\u001b[0m lstm_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(lstm_inputs)\n\u001b[1;32m    204\u001b[0m lstm_inputs \u001b[38;5;241m=\u001b[39m PackedSequence(lstm_inputs, inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_sizes)\n\u001b[0;32m--> 206\u001b[0m lstm_outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparserlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparserlstm_h_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparserlstm_c_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m lstm_outputs, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(lstm_outputs, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m unlabeled_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munlabeled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(lstm_outputs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(lstm_outputs))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/common/hlstm.py:104\u001b[0m, in \u001b[0;36mHighwayLSTM.forward\u001b[0;34m(self, input, seqlens, hx)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m PackedSequence(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdata), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sizes, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msorted_indices, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsorted_indices)\n\u001b[1;32m    103\u001b[0m layer_hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m][l \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions:(l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions], hx[\u001b[38;5;241m1\u001b[39m][l \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions:(l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_directions]) \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m h, (ht, ct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_hx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m hs\u001b[38;5;241m.\u001b[39mappend(ht)\n\u001b[1;32m    107\u001b[0m cs\u001b[38;5;241m.\u001b[39mappend(ct)\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/itba/2024C2/sia/tps/envtp1/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1135\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1125\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1147\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "\n",
    "flybondi_data = '../data/cleaned_with_translated_non_es_reviews.csv'\n",
    "df = pd.read_csv(flybondi_data)\n",
    "\n",
    "stanza.download('es') \n",
    "nlp = stanza.Pipeline('es')\n",
    "\n",
    "def lemmatize_spanish(text):\n",
    "    doc = nlp(text)  \n",
    "    \n",
    "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "# Apply the lemmatization function only to Spanish reviews\n",
    "df['review_lemmatized'] = df.apply(\n",
    "    lambda row: lemmatize_spanish(row['review_translated']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the DataFrame with the original, translated, and lemmatized reviews\n",
    "print(df[['review_translated', 'language', 'review_lemmatized']])\n",
    "\n",
    "# Optionally, save the updated DataFrame with lemmatized reviews\n",
    "output_file_lemmatized_reviews = 'cleaned_with_lemmatized_reviews.csv'\n",
    "df.to_csv(output_file_lemmatized_reviews, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
